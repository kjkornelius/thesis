{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inlin\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/Kornelius/Desktop/Data 2/nips-papers/papers.csv', header = 0, sep = ';', error_bad_lines=False)\n",
    "# Drop the columns not needed\n",
    "\n",
    "df = df[df.abstract != 'Abstract Missing']\n",
    "df = df.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(data):\n",
    "    stop_free = \" \".join([i for i in data.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data = [clean(data).split() for data in data]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove characters and numbers\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data)) # list in front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16129 unique tokens: ['activity', 'also', 'analysis', 'applied', 'assume']...)\n"
     ]
    }
   ],
   "source": [
    "data = data_words\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16119 unique tokens: ['activity', 'also', 'analysis', 'applied', 'assume']...)\n"
     ]
    }
   ],
   "source": [
    "id2word.filter_n_most_frequent(10)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data\n",
    "# Term Document Frequency and creating corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model20 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model50 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model100 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=100, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.006*\"function\" + 0.005*\"propose\" + 0.004*\"using\" + 0.004*\"bound\" + '\n",
      "  '0.004*\"study\" + 0.004*\"network\" + 0.004*\"distribution\" + 0.004*\"new\" + '\n",
      "  '0.004*\"image\" + 0.004*\"based\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"regret\" + 0.015*\"policy\" + 0.013*\"decision\" + 0.013*\"bandit\" + '\n",
      "  '0.013*\"action\" + 0.010*\"reward\" + 0.009*\"agent\" + 0.009*\"game\" + '\n",
      "  '0.008*\"feedback\" + 0.008*\"setting\"'),\n",
      " (1,\n",
      "  '0.025*\"matrix\" + 0.024*\"graph\" + 0.016*\"structure\" + 0.015*\"variable\" + '\n",
      "  '0.011*\"latent\" + 0.010*\"word\" + 0.010*\"observation\" + 0.009*\"signal\" + '\n",
      "  '0.009*\"topic\" + 0.008*\"sparse\"'),\n",
      " (2,\n",
      "  '0.013*\"image\" + 0.013*\"task\" + 0.012*\"network\" + 0.010*\"feature\" + '\n",
      "  '0.009*\"inference\" + 0.009*\"label\" + 0.008*\"training\" + 0.008*\"using\" + '\n",
      "  '0.007*\"propose\" + 0.006*\"use\"'),\n",
      " (3,\n",
      "  '0.012*\"function\" + 0.009*\"bound\" + 0.007*\"study\" + 0.006*\"number\" + '\n",
      "  '0.006*\"distribution\" + 0.006*\"sample\" + 0.006*\"linear\" + '\n",
      "  '0.006*\"optimization\" + 0.005*\"also\" + 0.005*\"analysis\"'),\n",
      " (4,\n",
      "  '0.014*\"neuron\" + 0.013*\"memory\" + 0.013*\"neural\" + 0.012*\"dynamic\" + '\n",
      "  '0.011*\"network\" + 0.011*\"system\" + 0.010*\"brain\" + 0.009*\"spike\" + '\n",
      "  '0.008*\"state\" + 0.007*\"signal\"')]\n",
      "[(0,\n",
      "  '0.030*\"word\" + 0.025*\"policy\" + 0.021*\"decision\" + 0.020*\"topic\" + '\n",
      "  '0.018*\"reward\" + 0.015*\"subset\" + 0.013*\"state\" + 0.013*\"optimal\" + '\n",
      "  '0.012*\"score\" + 0.012*\"submodular\"'),\n",
      " (1,\n",
      "  '0.021*\"game\" + 0.019*\"action\" + 0.019*\"agent\" + 0.016*\"expert\" + '\n",
      "  '0.015*\"tensor\" + 0.013*\"strategy\" + 0.013*\"player\" + 0.013*\"environment\" + '\n",
      "  '0.012*\"behavior\" + 0.012*\"interaction\"'),\n",
      " (2,\n",
      "  '0.038*\"inference\" + 0.031*\"process\" + 0.025*\"bayesian\" + 0.018*\"latent\" + '\n",
      "  '0.016*\"variational\" + 0.015*\"probabilistic\" + 0.014*\"state\" + '\n",
      "  '0.014*\"sampling\" + 0.013*\"posterior\" + 0.012*\"markov\"'),\n",
      " (3,\n",
      "  '0.025*\"task\" + 0.024*\"feature\" + 0.015*\"memory\" + 0.014*\"clustering\" + '\n",
      "  '0.012*\"weight\" + 0.012*\"sequence\" + 0.009*\"within\" + 0.008*\"layer\" + '\n",
      "  '0.007*\"input\" + 0.007*\"discrete\"'),\n",
      " (4,\n",
      "  '0.037*\"signal\" + 0.024*\"neuron\" + 0.017*\"brain\" + 0.016*\"neural\" + '\n",
      "  '0.015*\"system\" + 0.015*\"dynamic\" + 0.015*\"spike\" + 0.012*\"trajectory\" + '\n",
      "  '0.012*\"functional\" + 0.010*\"activity\"'),\n",
      " (5,\n",
      "  '0.011*\"study\" + 0.011*\"propose\" + 0.009*\"based\" + 0.009*\"framework\" + '\n",
      "  '0.009*\"set\" + 0.009*\"new\" + 0.009*\"performance\" + 0.008*\"number\" + '\n",
      "  '0.008*\"using\" + 0.008*\"also\"'),\n",
      " (6,\n",
      "  '0.059*\"network\" + 0.035*\"label\" + 0.025*\"training\" + 0.023*\"deep\" + '\n",
      "  '0.020*\"neural\" + 0.018*\"task\" + 0.017*\"image\" + 0.015*\"representation\" + '\n",
      "  '0.013*\"learn\" + 0.012*\"translation\"'),\n",
      " (7,\n",
      "  '0.053*\"image\" + 0.041*\"regret\" + 0.032*\"object\" + 0.023*\"bandit\" + '\n",
      "  '0.019*\"human\" + 0.017*\"visual\" + 0.014*\"feedback\" + 0.011*\"context\" + '\n",
      "  '0.011*\"detection\" + 0.011*\"arm\"'),\n",
      " (8,\n",
      "  '0.042*\"distribution\" + 0.030*\"structure\" + 0.027*\"graph\" + '\n",
      "  '0.018*\"estimator\" + 0.017*\"estimation\" + 0.014*\"family\" + 0.014*\"variable\" '\n",
      "  '+ 0.012*\"density\" + 0.011*\"estimate\" + 0.010*\"parameter\"'),\n",
      " (9,\n",
      "  '0.024*\"function\" + 0.021*\"matrix\" + 0.018*\"bound\" + 0.018*\"optimization\" + '\n",
      "  '0.015*\"stochastic\" + 0.014*\"gradient\" + 0.014*\"rate\" + 0.013*\"linear\" + '\n",
      "  '0.013*\"convergence\" + 0.013*\"loss\"')]\n",
      "[(0,\n",
      "  '0.052*\"policy\" + 0.046*\"decision\" + 0.031*\"agent\" + 0.025*\"arm\" + '\n",
      "  '0.025*\"submodular\" + 0.020*\"reinforcement\" + 0.018*\"optimal\" + '\n",
      "  '0.018*\"value\" + 0.015*\"inequality\" + 0.014*\"tight\"'),\n",
      " (1,\n",
      "  '0.046*\"state\" + 0.036*\"context\" + 0.028*\"action\" + 0.025*\"sequence\" + '\n",
      "  '0.022*\"reward\" + 0.021*\"behavior\" + 0.019*\"good\" + 0.018*\"observation\" + '\n",
      "  '0.016*\"hidden\" + 0.015*\"oracle\"'),\n",
      " (2,\n",
      "  '0.040*\"markov\" + 0.025*\"global\" + 0.024*\"continuous\" + 0.022*\"recover\" + '\n",
      "  '0.022*\"chain\" + 0.021*\"sampler\" + 0.017*\"basis\" + 0.016*\"monte\" + '\n",
      "  '0.016*\"carlo\" + 0.015*\"transition\"'),\n",
      " (3,\n",
      "  '0.062*\"memory\" + 0.049*\"component\" + 0.023*\"alternating\" + 0.020*\"game\" + '\n",
      "  '0.014*\"principal\" + 0.014*\"explain\" + 0.014*\"become\" + 0.013*\"regime\" + '\n",
      "  '0.012*\"side\" + 0.012*\"boosting\"'),\n",
      " (4,\n",
      "  '0.091*\"system\" + 0.043*\"dynamic\" + 0.025*\"trajectory\" + 0.020*\"minimizing\" '\n",
      "  '+ 0.017*\"limited\" + 0.017*\"cognitive\" + 0.016*\"filter\" + 0.014*\"equation\" + '\n",
      "  '0.014*\"temporal\" + 0.013*\"answer\"'),\n",
      " (5,\n",
      "  '0.027*\"function\" + 0.023*\"study\" + 0.023*\"bound\" + 0.016*\"setting\" + '\n",
      "  '0.016*\"time\" + 0.014*\"analysis\" + 0.014*\"number\" + 0.013*\"sample\" + '\n",
      "  '0.013*\"optimization\" + 0.013*\"provide\"'),\n",
      " (6,\n",
      "  '0.089*\"network\" + 0.079*\"image\" + 0.036*\"deep\" + 0.033*\"representation\" + '\n",
      "  '0.033*\"object\" + 0.028*\"neural\" + 0.028*\"training\" + 0.018*\"feature\" + '\n",
      "  '0.018*\"task\" + 0.015*\"architecture\"'),\n",
      " (7,\n",
      "  '0.073*\"regret\" + 0.041*\"bandit\" + 0.025*\"feedback\" + 0.025*\"parallel\" + '\n",
      "  '0.017*\"best\" + 0.016*\"player\" + 0.015*\"strategy\" + 0.013*\"run\" + '\n",
      "  '0.012*\"worstcase\" + 0.012*\"come\"'),\n",
      " (8,\n",
      "  '0.042*\"estimator\" + 0.037*\"estimation\" + 0.023*\"signal\" + 0.023*\"unknown\" + '\n",
      "  '0.018*\"noise\" + 0.017*\"consistency\" + 0.016*\"condition\" + 0.016*\"mean\" + '\n",
      "  '0.016*\"graphical\" + 0.015*\"variance\"'),\n",
      " (9,\n",
      "  '0.041*\"graph\" + 0.041*\"linear\" + 0.029*\"sparse\" + 0.025*\"prove\" + '\n",
      "  '0.023*\"solution\" + 0.019*\"dimension\" + 0.019*\"property\" + 0.019*\"objective\" '\n",
      "  '+ 0.018*\"metric\" + 0.013*\"optimal\"'),\n",
      " (10,\n",
      "  '0.109*\"information\" + 0.035*\"item\" + 0.033*\"lower\" + 0.024*\"preference\" + '\n",
      "  '0.023*\"upper\" + 0.022*\"communication\" + 0.019*\"adversarial\" + '\n",
      "  '0.018*\"minimax\" + 0.013*\"retrieval\" + 0.013*\"together\"'),\n",
      " (11,\n",
      "  '0.052*\"human\" + 0.038*\"visual\" + 0.029*\"mechanism\" + 0.025*\"recognition\" + '\n",
      "  '0.024*\"working\" + 0.023*\"account\" + 0.022*\"effect\" + 0.015*\"region\" + '\n",
      "  '0.014*\"spatial\" + 0.013*\"hundred\"'),\n",
      " (12,\n",
      "  '0.018*\"propose\" + 0.015*\"using\" + 0.015*\"framework\" + 0.014*\"new\" + '\n",
      "  '0.013*\"present\" + 0.012*\"proposed\" + 0.012*\"novel\" + 0.012*\"set\" + '\n",
      "  '0.012*\"task\" + 0.011*\"performance\"'),\n",
      " (13,\n",
      "  '0.098*\"gradient\" + 0.051*\"descent\" + 0.037*\"translation\" + '\n",
      "  '0.026*\"distributed\" + 0.017*\"typically\" + 0.012*\"tracking\" + '\n",
      "  '0.012*\"accelerated\" + 0.011*\"sgd\" + 0.011*\"global\" + 0.010*\"surrogate\"'),\n",
      " (14,\n",
      "  '0.038*\"weight\" + 0.034*\"input\" + 0.029*\"convolutional\" + 0.028*\"connection\" '\n",
      "  '+ 0.026*\"source\" + 0.025*\"layer\" + 0.023*\"score\" + 0.023*\"rule\" + '\n",
      "  '0.021*\"natural\" + 0.018*\"element\"'),\n",
      " (15,\n",
      "  '0.031*\"neuron\" + 0.024*\"neural\" + 0.022*\"pattern\" + 0.022*\"brain\" + '\n",
      "  '0.020*\"gp\" + 0.019*\"spike\" + 0.018*\"signal\" + 0.016*\"response\" + '\n",
      "  '0.015*\"functional\" + 0.014*\"connectivity\"'),\n",
      " (16,\n",
      "  '0.038*\"search\" + 0.033*\"active\" + 0.029*\"query\" + 0.028*\"hashing\" + '\n",
      "  '0.025*\"learner\" + 0.019*\"annotator\" + 0.018*\"fact\" + 0.017*\"total\" + '\n",
      "  '0.015*\"path\" + 0.014*\"collection\"'),\n",
      " (17,\n",
      "  '0.062*\"clustering\" + 0.040*\"cluster\" + 0.035*\"group\" + 0.026*\"detection\" + '\n",
      "  '0.019*\"segmentation\" + 0.016*\"kmeans\" + 0.015*\"speech\" + 0.015*\"detect\" + '\n",
      "  '0.014*\"margin\" + 0.014*\"initialization\"'),\n",
      " (18,\n",
      "  '0.048*\"distribution\" + 0.030*\"inference\" + 0.025*\"process\" + '\n",
      "  '0.023*\"variable\" + 0.021*\"bayesian\" + 0.020*\"kernel\" + 0.016*\"gaussian\" + '\n",
      "  '0.015*\"latent\" + 0.014*\"prior\" + 0.014*\"density\"'),\n",
      " (19,\n",
      "  '0.081*\"matrix\" + 0.051*\"loss\" + 0.043*\"convex\" + 0.035*\"regularization\" + '\n",
      "  '0.025*\"norm\" + 0.025*\"rank\" + 0.024*\"risk\" + 0.021*\"minimization\" + '\n",
      "  '0.020*\"version\" + 0.013*\"nonconvex\"')]\n",
      "[(42,\n",
      "  '0.077*\"language\" + 0.059*\"typically\" + 0.035*\"alignment\" + '\n",
      "  '0.027*\"reconstruct\" + 0.027*\"recurrent\" + 0.022*\"skill\" + 0.019*\"actively\" '\n",
      "  '+ 0.013*\"controlled\" + 0.012*\"gru\" + 0.012*\"showed\"'),\n",
      " (0,\n",
      "  '0.053*\"annotator\" + 0.050*\"programming\" + 0.047*\"run\" + 0.043*\"worstcase\" + '\n",
      "  '0.039*\"explain\" + 0.035*\"them\" + 0.021*\"suggested\" + 0.018*\"mirror\" + '\n",
      "  '0.013*\"distortion\" + 0.013*\"stock\"'),\n",
      " (33,\n",
      "  '0.206*\"sampling\" + 0.051*\"importance\" + 0.032*\"alpha\" + '\n",
      "  '0.028*\"demonstrating\" + 0.024*\"scalability\" + 0.024*\"ever\" + 0.022*\"svms\" + '\n",
      "  '0.019*\"approximates\" + 0.017*\"curvature\" + 0.016*\"manual\"'),\n",
      " (37,\n",
      "  '0.086*\"convolutional\" + 0.076*\"layer\" + 0.044*\"injection\" + '\n",
      "  '0.031*\"recurrent\" + 0.030*\"input\" + 0.025*\"encoding\" + 0.024*\"scene\" + '\n",
      "  '0.023*\"connection\" + 0.021*\"temporally\" + 0.019*\"switching\"'),\n",
      " (49,\n",
      "  '0.078*\"mechanism\" + 0.054*\"working\" + 0.033*\"nature\" + '\n",
      "  '0.032*\"precipitation\" + 0.032*\"nowcasting\" + 0.026*\"heldout\" + '\n",
      "  '0.026*\"event\" + 0.024*\"dbh\" + 0.023*\"big\" + 0.019*\"ratio\"'),\n",
      " (36,\n",
      "  '0.080*\"generalized\" + 0.048*\"partial\" + 0.042*\"gamma\" + 0.038*\"explicitly\" '\n",
      "  '+ 0.029*\"service\" + 0.026*\"infer\" + 0.026*\"direct\" + 0.022*\"cascade\" + '\n",
      "  '0.019*\"viral\" + 0.017*\"technical\"'),\n",
      " (46,\n",
      "  '0.146*\"policy\" + 0.094*\"search\" + 0.053*\"achieving\" + 0.036*\"value\" + '\n",
      "  '0.032*\"reinforcement\" + 0.027*\"nmt\" + 0.022*\"otherwise\" + 0.021*\"reducing\" '\n",
      "  '+ 0.021*\"suboptimal\" + 0.020*\"latter\"'),\n",
      " (17,\n",
      "  '0.116*\"clustering\" + 0.076*\"dual\" + 0.074*\"cluster\" + 0.070*\"polynomial\" + '\n",
      "  '0.030*\"kmeans\" + 0.027*\"guaranteed\" + 0.025*\"initialization\" + '\n",
      "  '0.024*\"primal\" + 0.024*\"includes\" + 0.024*\"center\"'),\n",
      " (10,\n",
      "  '0.116*\"structured\" + 0.065*\"idea\" + 0.061*\"benefit\" + 0.047*\"scalable\" + '\n",
      "  '0.036*\"still\" + 0.035*\"gating\" + 0.031*\"prediction\" + 0.026*\"mutual\" + '\n",
      "  '0.022*\"inference\" + 0.022*\"output\"'),\n",
      " (45,\n",
      "  '0.084*\"continuous\" + 0.079*\"discrete\" + 0.077*\"rule\" + 0.054*\"belief\" + '\n",
      "  '0.046*\"propagation\" + 0.038*\"uncertainty\" + 0.033*\"intractable\" + '\n",
      "  '0.025*\"hence\" + 0.021*\"planning\" + 0.018*\"interval\"'),\n",
      " (5,\n",
      "  '0.062*\"multiple\" + 0.059*\"robust\" + 0.046*\"realworld\" + 0.042*\"selection\" + '\n",
      "  '0.032*\"source\" + 0.028*\"weighted\" + 0.027*\"unsupervised\" + '\n",
      "  '0.026*\"semisupervised\" + 0.025*\"effectiveness\" + 0.023*\"million\"'),\n",
      " (34,\n",
      "  '0.022*\"unit\" + 0.020*\"alternating\" + 0.019*\"player\" + 0.018*\"eg\" + '\n",
      "  '0.016*\"define\" + 0.015*\"treewidth\" + 0.014*\"require\" + 0.014*\"retrieval\" + '\n",
      "  '0.013*\"comparable\" + 0.012*\"party\"'),\n",
      " (15,\n",
      "  '0.056*\"system\" + 0.040*\"memory\" + 0.036*\"dynamic\" + 0.034*\"probabilistic\" + '\n",
      "  '0.033*\"sequence\" + 0.024*\"type\" + 0.023*\"pattern\" + 0.022*\"observed\" + '\n",
      "  '0.021*\"feedback\" + 0.020*\"gp\"'),\n",
      " (23,\n",
      "  '0.024*\"provides\" + 0.020*\"practical\" + 0.019*\"individual\" + 0.016*\"close\" + '\n",
      "  '0.016*\"accurate\" + 0.015*\"popular\" + 0.015*\"potential\" + 0.014*\"exhibit\" + '\n",
      "  '0.013*\"depends\" + 0.013*\"filter\"'),\n",
      " (30,\n",
      "  '0.165*\"function\" + 0.121*\"bound\" + 0.060*\"error\" + 0.054*\"approximation\" + '\n",
      "  '0.045*\"family\" + 0.044*\"point\" + 0.042*\"objective\" + 0.037*\"generalization\" '\n",
      "  '+ 0.028*\"exponential\" + 0.025*\"empirically\"'),\n",
      " (18,\n",
      "  '0.108*\"distribution\" + 0.073*\"sample\" + 0.047*\"random\" + '\n",
      "  '0.041*\"probability\" + 0.039*\"given\" + 0.031*\"mean\" + 0.031*\"measure\" + '\n",
      "  '0.031*\"density\" + 0.030*\"test\" + 0.025*\"unknown\"'),\n",
      " (19,\n",
      "  '0.080*\"study\" + 0.060*\"time\" + 0.037*\"loss\" + 0.035*\"case\" + '\n",
      "  '0.033*\"setting\" + 0.032*\"guarantee\" + 0.032*\"theoretical\" + 0.024*\"order\" + '\n",
      "  '0.023*\"provide\" + 0.023*\"first\"'),\n",
      " (2,\n",
      "  '0.036*\"linear\" + 0.034*\"number\" + 0.032*\"set\" + 0.028*\"space\" + '\n",
      "  '0.027*\"optimal\" + 0.022*\"regression\" + 0.022*\"efficient\" + 0.021*\"prove\" + '\n",
      "  '0.020*\"solution\" + 0.019*\"consider\"'),\n",
      " (3,\n",
      "  '0.031*\"framework\" + 0.031*\"analysis\" + 0.029*\"parameter\" + 0.026*\"general\" '\n",
      "  '+ 0.023*\"based\" + 0.022*\"application\" + 0.022*\"optimization\" + '\n",
      "  '0.020*\"sparse\" + 0.019*\"propose\" + 0.019*\"structure\"'),\n",
      " (12,\n",
      "  '0.022*\"image\" + 0.022*\"task\" + 0.020*\"performance\" + 0.019*\"using\" + '\n",
      "  '0.018*\"feature\" + 0.018*\"present\" + 0.017*\"class\" + 0.016*\"use\" + '\n",
      "  '0.016*\"new\" + 0.016*\"two\"')]\n",
      "[(43,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (72,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (30,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (75,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (91,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (76,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (32,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (42,\n",
      "  '0.000*\"ocument\" + 0.000*\"disclda\" + 0.000*\"fsrnns\" + 0.000*\"fsrnn\" + '\n",
      "  '0.000*\"fastslow\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"newsgroup\" + '\n",
      "  '0.000*\"composable\" + 0.000*\"satellite\"'),\n",
      " (45,\n",
      "  '0.062*\"linguistic\" + 0.000*\"bpc\" + 0.000*\"abundant\" + 0.000*\"hutter\" + '\n",
      "  '0.000*\"fsrnns\" + 0.000*\"fsrnn\" + 0.000*\"fastslow\" + 0.000*\"constituting\" + '\n",
      "  '0.000*\"disclda\" + 0.000*\"classdependent\"'),\n",
      " (55,\n",
      "  '0.105*\"encountered\" + 0.000*\"bpc\" + 0.000*\"ocument\" + 0.000*\"hutter\" + '\n",
      "  '0.000*\"fsrnns\" + 0.000*\"fsrnn\" + 0.000*\"fastslow\" + 0.000*\"amino\" + '\n",
      "  '0.000*\"classdependent\" + 0.000*\"satellite\"'),\n",
      " (89,\n",
      "  '0.192*\"rate\" + 0.169*\"convergence\" + 0.140*\"convex\" + 0.069*\"minimization\" '\n",
      "  '+ 0.040*\"projection\" + 0.032*\"optimization\" + 0.032*\"sum\" + 0.031*\"smooth\" '\n",
      "  '+ 0.027*\"hard\" + 0.022*\"accelerated\"'),\n",
      " (3,\n",
      "  '0.287*\"network\" + 0.112*\"deep\" + 0.111*\"neural\" + 0.086*\"memory\" + '\n",
      "  '0.046*\"layer\" + 0.042*\"hidden\" + 0.036*\"unit\" + 0.030*\"recurrent\" + '\n",
      "  '0.018*\"regime\" + 0.017*\"side\"'),\n",
      " (19,\n",
      "  '0.118*\"guarantee\" + 0.097*\"sampling\" + 0.088*\"order\" + 0.068*\"norm\" + '\n",
      "  '0.066*\"give\" + 0.053*\"faster\" + 0.045*\"main\" + 0.037*\"requires\" + '\n",
      "  '0.036*\"instead\" + 0.036*\"exploit\"'),\n",
      " (86,\n",
      "  '0.136*\"complexity\" + 0.107*\"cost\" + 0.098*\"consider\" + 0.085*\"high\" + '\n",
      "  '0.082*\"computational\" + 0.067*\"via\" + 0.055*\"available\" + '\n",
      "  '0.050*\"computation\" + 0.036*\"automatically\" + 0.030*\"dimensional\"'),\n",
      " (71,\n",
      "  '0.163*\"analysis\" + 0.117*\"error\" + 0.105*\"approximation\" + '\n",
      "  '0.098*\"theoretical\" + 0.087*\"family\" + 0.086*\"point\" + 0.071*\"standard\" + '\n",
      "  '0.056*\"exponential\" + 0.035*\"yield\" + 0.034*\"finally\"'),\n",
      " (27,\n",
      "  '0.175*\"study\" + 0.096*\"provide\" + 0.085*\"case\" + 0.067*\"given\" + '\n",
      "  '0.049*\"known\" + 0.049*\"particular\" + 0.025*\"improved\" + 0.025*\"applied\" + '\n",
      "  '0.023*\"useful\" + 0.021*\"interest\"'),\n",
      " (2,\n",
      "  '0.072*\"develop\" + 0.066*\"prove\" + 0.061*\"property\" + 0.052*\"recent\" + '\n",
      "  '0.049*\"constraint\" + 0.043*\"provides\" + 0.041*\"design\" + 0.039*\"may\" + '\n",
      "  '0.039*\"achieves\" + 0.035*\"knowledge\"'),\n",
      " (14,\n",
      "  '0.068*\"task\" + 0.061*\"performance\" + 0.054*\"new\" + 0.040*\"training\" + '\n",
      "  '0.039*\"several\" + 0.037*\"proposed\" + 0.037*\"application\" + '\n",
      "  '0.037*\"classification\" + 0.035*\"machine\" + 0.032*\"many\"'),\n",
      " (15,\n",
      "  '0.045*\"using\" + 0.040*\"two\" + 0.039*\"number\" + 0.037*\"present\" + '\n",
      "  '0.031*\"use\" + 0.030*\"experiment\" + 0.030*\"one\" + 0.027*\"large\" + '\n",
      "  '0.024*\"demonstrate\" + 0.024*\"used\"'),\n",
      " (31,\n",
      "  '0.045*\"propose\" + 0.040*\"set\" + 0.040*\"also\" + 0.040*\"framework\" + '\n",
      "  '0.036*\"setting\" + 0.035*\"based\" + 0.033*\"linear\" + 0.031*\"parameter\" + '\n",
      "  '0.028*\"optimal\" + 0.028*\"efficient\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model5.print_topics())\n",
    "doc_lda = lda_model5[corpus]\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model10.print_topics())\n",
    "doc_lda = lda_model10[corpus]\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model20.print_topics())\n",
    "doc_lda = lda_model20[corpus]\n",
    "\n",
    "# Print the Keyword in the 50 topics\n",
    "pprint(lda_model50.print_topics())\n",
    "doc_lda = lda_model50[corpus]\n",
    "\n",
    "# Print the Keyword in the 100 topics\n",
    "pprint(lda_model100.print_topics())\n",
    "doc_lda = lda_model100[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.9932228663998215\n",
      "\n",
      "Perplexity5:  -7.9961106070945\n",
      "\n",
      "Perplexity10:  -8.097161251693016\n",
      "\n",
      "Perplexity20:  -8.130576958889495\n",
      "\n",
      "Perplexity50:  -8.184060261436242\n",
      "\n",
      "Perplexity100:  -28.582179719467995\n",
      "\n",
      "Coherence Score:  0.20440402452744003\n",
      "\n",
      "Coherence Score5:  0.41507579681546564\n",
      "\n",
      "Coherence Score10:  0.4178097625349243\n",
      "\n",
      "Coherence Score20:  0.3913498592931872\n",
      "\n",
      "Coherence Score50:  0.41310656146633545\n",
      "\n",
      "Coherence Score100:  0.373522228928749\n",
      "\n",
      "Coherence ScoreUMass:  -1.6898114615844813\n",
      "\n",
      "Coherence Score5UMass:  -2.3740495371855204\n",
      "\n",
      "Coherence Score10UMass:  -3.6026646812892897\n",
      "\n",
      "Coherence Score20UMass:  -6.2128712204120164\n",
      "\n",
      "Coherence Score50UMass:  -9.414870554517266\n",
      "\n",
      "Coherence Score100UMass:  -10.911420695376908\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity5: ', lda_model5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity10: ', lda_model10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity20: ', lda_model20.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity50: ', lda_model50.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity100: ', lda_model100.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100: ', coherence_lda)\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence ScoreUMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100UMass: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
