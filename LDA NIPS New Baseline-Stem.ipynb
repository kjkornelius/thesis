{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inlin\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/Kornelius/Desktop/Data 2/nips-papers/papers.csv', header = 0, sep = ';', error_bad_lines=False)\n",
    "# Drop the columns not needed\n",
    "df = df.drop(columns=['id', 'event_type', 'pdf_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row by condition\n",
    "df = df[df.abstract != 'Abstract Missing']\n",
    "# Print out the first rows of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.abstract.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "ps = PorterStemmer()\n",
    "def clean(data):\n",
    "    stop_free = \" \".join([i for i in data.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(ps.stem(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data = [clean(data).split() for data in data]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove characters and numbers\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12827 unique tokens: ['activ', 'also', 'analysi', 'appli', 'assum']...)\n"
     ]
    }
   ],
   "source": [
    "data = data_words\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data\n",
    "# Term Document Frequency and creating corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model20 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model50 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model100 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=100, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"model\" + 0.013*\"algorithm\" + 0.013*\"learn\" + 0.010*\"use\" + '\n",
      "  '0.010*\"problem\" + 0.010*\"method\" + 0.008*\"data\" + 0.008*\"show\" + '\n",
      "  '0.007*\"propos\" + 0.007*\"gener\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 1 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.029*\"bound\" + 0.025*\"algorithm\" + 0.016*\"regret\" + 0.015*\"learn\" + '\n",
      "  '0.015*\"problem\" + 0.013*\"set\" + 0.011*\"optim\" + 0.010*\"onlin\" + '\n",
      "  '0.010*\"studi\" + 0.010*\"polici\"'),\n",
      " (1,\n",
      "  '0.018*\"algorithm\" + 0.016*\"method\" + 0.014*\"problem\" + 0.012*\"learn\" + '\n",
      "  '0.012*\"use\" + 0.011*\"propos\" + 0.010*\"show\" + 0.010*\"estim\" + 0.010*\"data\" '\n",
      "  '+ 0.010*\"model\"'),\n",
      " (2,\n",
      "  '0.042*\"model\" + 0.014*\"system\" + 0.012*\"data\" + 0.011*\"activ\" + 0.011*\"use\" '\n",
      "  '+ 0.010*\"dynam\" + 0.010*\"network\" + 0.009*\"process\" + 0.009*\"observ\" + '\n",
      "  '0.009*\"signal\"'),\n",
      " (3,\n",
      "  '0.034*\"graph\" + 0.034*\"matrix\" + 0.027*\"cluster\" + 0.019*\"rank\" + '\n",
      "  '0.009*\"algorithm\" + 0.009*\"complet\" + 0.008*\"recov\" + 0.008*\"node\" + '\n",
      "  '0.008*\"norm\" + 0.008*\"matric\"'),\n",
      " (4,\n",
      "  '0.027*\"learn\" + 0.023*\"imag\" + 0.021*\"model\" + 0.019*\"train\" + '\n",
      "  '0.019*\"label\" + 0.018*\"network\" + 0.016*\"task\" + 0.012*\"use\" + '\n",
      "  '0.011*\"object\" + 0.010*\"featur\"')]\n",
      "[(0,\n",
      "  '0.030*\"submodular\" + 0.021*\"rule\" + 0.020*\"search\" + 0.018*\"belief\" + '\n",
      "  '0.017*\"code\" + 0.017*\"influenc\" + 0.013*\"entropi\" + 0.012*\"analyt\" + '\n",
      "  '0.011*\"flow\" + 0.010*\"heldout\"'),\n",
      " (1,\n",
      "  '0.043*\"model\" + 0.036*\"learn\" + 0.025*\"use\" + 0.024*\"data\" + 0.022*\"method\" '\n",
      "  '+ 0.018*\"propos\" + 0.017*\"approach\" + 0.012*\"gener\" + 0.011*\"structur\" + '\n",
      "  '0.010*\"label\"'),\n",
      " (2,\n",
      "  '0.025*\"activ\" + 0.023*\"system\" + 0.021*\"model\" + 0.019*\"dynam\" + '\n",
      "  '0.018*\"neuron\" + 0.016*\"observ\" + 0.015*\"spike\" + 0.015*\"signal\" + '\n",
      "  '0.014*\"human\" + 0.014*\"state\"'),\n",
      " (3,\n",
      "  '0.020*\"game\" + 0.017*\"player\" + 0.014*\"agent\" + 0.012*\"actual\" + '\n",
      "  '0.012*\"nowcast\" + 0.012*\"precipit\" + 0.011*\"fact\" + 0.011*\"parti\" + '\n",
      "  '0.011*\"help\" + 0.010*\"fair\"'),\n",
      " (4,\n",
      "  '0.060*\"network\" + 0.060*\"imag\" + 0.028*\"train\" + 0.026*\"deep\" + '\n",
      "  '0.023*\"word\" + 0.023*\"object\" + 0.021*\"neural\" + 0.014*\"translat\" + '\n",
      "  '0.014*\"convolut\" + 0.013*\"represent\"'),\n",
      " (5,\n",
      "  '0.074*\"graph\" + 0.027*\"convex\" + 0.023*\"smooth\" + 0.019*\"acceler\" + '\n",
      "  '0.018*\"regular\" + 0.018*\"manifold\" + 0.017*\"converg\" + 0.017*\"relax\" + '\n",
      "  '0.015*\"descent\" + 0.013*\"node\"'),\n",
      " (6,\n",
      "  '0.033*\"algorithm\" + 0.026*\"problem\" + 0.016*\"estim\" + 0.015*\"function\" + '\n",
      "  '0.014*\"show\" + 0.013*\"result\" + 0.012*\"set\" + 0.012*\"optim\" + 0.011*\"bound\" '\n",
      "  '+ 0.010*\"studi\"'),\n",
      " (7,\n",
      "  '0.027*\"topic\" + 0.023*\"hash\" + 0.023*\"binari\" + 0.023*\"commun\" + '\n",
      "  '0.023*\"item\" + 0.016*\"feedback\" + 0.014*\"privaci\" + 0.014*\"context\" + '\n",
      "  '0.012*\"minimax\" + 0.011*\"inject\"'),\n",
      " (8,\n",
      "  '0.033*\"stochast\" + 0.028*\"onlin\" + 0.027*\"regret\" + 0.026*\"gradient\" + '\n",
      "  '0.023*\"algorithm\" + 0.020*\"learn\" + 0.018*\"optim\" + 0.017*\"memori\" + '\n",
      "  '0.017*\"polici\" + 0.015*\"bandit\"'),\n",
      " (9,\n",
      "  '0.033*\"cluster\" + 0.032*\"embed\" + 0.021*\"dimens\" + 0.019*\"metric\" + '\n",
      "  '0.019*\"data\" + 0.017*\"rank\" + 0.015*\"relat\" + 0.014*\"tree\" + '\n",
      "  '0.013*\"similar\" + 0.013*\"group\"')]\n",
      "[(0,\n",
      "  '0.177*\"featur\" + 0.050*\"code\" + 0.039*\"spars\" + 0.028*\"encod\" + '\n",
      "  '0.025*\"pairwis\" + 0.024*\"influenc\" + 0.022*\"vector\" + 0.015*\"substanti\" + '\n",
      "  '0.015*\"field\" + 0.015*\"similar\"'),\n",
      " (1,\n",
      "  '0.071*\"state\" + 0.034*\"sequenc\" + 0.032*\"markov\" + 0.029*\"continu\" + '\n",
      "  '0.028*\"likelihood\" + 0.024*\"discret\" + 0.023*\"art\" + 0.021*\"dynam\" + '\n",
      "  '0.019*\"recurr\" + 0.018*\"chain\"'),\n",
      " (2,\n",
      "  '0.062*\"activ\" + 0.030*\"human\" + 0.029*\"brain\" + 0.029*\"behavior\" + '\n",
      "  '0.026*\"prefer\" + 0.024*\"system\" + 0.024*\"dynam\" + 0.020*\"trajectori\" + '\n",
      "  '0.019*\"connect\" + 0.017*\"pattern\"'),\n",
      " (3,\n",
      "  '0.039*\"game\" + 0.031*\"agent\" + 0.023*\"player\" + 0.017*\"actual\" + '\n",
      "  '0.015*\"parti\" + 0.015*\"enough\" + 0.015*\"help\" + 0.013*\"plan\" + '\n",
      "  '0.013*\"protocol\" + 0.012*\"progress\"'),\n",
      " (4,\n",
      "  '0.179*\"network\" + 0.068*\"deep\" + 0.060*\"neural\" + 0.034*\"train\" + '\n",
      "  '0.033*\"convolut\" + 0.030*\"architectur\" + 0.030*\"layer\" + 0.016*\"connect\" + '\n",
      "  '0.015*\"treewidth\" + 0.015*\"unit\"'),\n",
      " (5,\n",
      "  '0.061*\"converg\" + 0.049*\"gradient\" + 0.048*\"rate\" + 0.046*\"regular\" + '\n",
      "  '0.043*\"stochast\" + 0.042*\"method\" + 0.040*\"convex\" + 0.031*\"optim\" + '\n",
      "  '0.025*\"descent\" + 0.019*\"smooth\"'),\n",
      " (6,\n",
      "  '0.050*\"algorithm\" + 0.036*\"problem\" + 0.021*\"show\" + 0.020*\"set\" + '\n",
      "  '0.020*\"result\" + 0.016*\"optim\" + 0.016*\"studi\" + 0.013*\"gener\" + '\n",
      "  '0.013*\"sampl\" + 0.011*\"number\"'),\n",
      " (7,\n",
      "  '0.151*\"graph\" + 0.074*\"dimens\" + 0.042*\"graphic\" + 0.036*\"sparsiti\" + '\n",
      "  '0.034*\"relax\" + 0.025*\"minimax\" + 0.021*\"edg\" + 0.020*\"align\" + '\n",
      "  '0.019*\"gamma\" + 0.014*\"attain\"'),\n",
      " (8,\n",
      "  '0.088*\"bound\" + 0.047*\"regret\" + 0.031*\"lower\" + 0.027*\"decis\" + '\n",
      "  '0.026*\"bandit\" + 0.026*\"action\" + 0.022*\"best\" + 0.020*\"adversari\" + '\n",
      "  '0.020*\"explor\" + 0.019*\"strategi\"'),\n",
      " (9,\n",
      "  '0.053*\"tree\" + 0.047*\"node\" + 0.035*\"structur\" + 0.032*\"predict\" + '\n",
      "  '0.029*\"video\" + 0.028*\"sampler\" + 0.022*\"motion\" + 0.021*\"futur\" + '\n",
      "  '0.019*\"precipit\" + 0.019*\"nowcast\"'),\n",
      " (10,\n",
      "  '0.101*\"cluster\" + 0.044*\"commun\" + 0.026*\"segment\" + 0.026*\"cost\" + '\n",
      "  '0.022*\"partit\" + 0.022*\"correct\" + 0.021*\"question\" + 0.020*\"user\" + '\n",
      "  '0.019*\"total\" + 0.017*\"answer\"'),\n",
      " (11,\n",
      "  '0.084*\"matrix\" + 0.038*\"norm\" + 0.033*\"rank\" + 0.023*\"feedback\" + '\n",
      "  '0.020*\"low\" + 0.019*\"complet\" + 0.019*\"upper\" + 0.017*\"nois\" + 0.017*\"et\" + '\n",
      "  '0.017*\"al\"'),\n",
      " (12,\n",
      "  '0.162*\"learn\" + 0.048*\"label\" + 0.042*\"task\" + 0.029*\"embed\" + '\n",
      "  '0.027*\"classif\" + 0.026*\"kernel\" + 0.021*\"machin\" + 0.019*\"train\" + '\n",
      "  '0.018*\"test\" + 0.016*\"data\"'),\n",
      " (13,\n",
      "  '0.047*\"polici\" + 0.043*\"valu\" + 0.041*\"control\" + 0.034*\"posterior\" + '\n",
      "  '0.032*\"reward\" + 0.032*\"parallel\" + 0.031*\"search\" + 0.025*\"scheme\" + '\n",
      "  '0.019*\"largescal\" + 0.019*\"approxim\"'),\n",
      " (14,\n",
      "  '0.049*\"model\" + 0.033*\"use\" + 0.027*\"data\" + 0.026*\"method\" + 0.020*\"estim\" '\n",
      "  '+ 0.018*\"propos\" + 0.015*\"approach\" + 0.012*\"structur\" + 0.011*\"gener\" + '\n",
      "  '0.011*\"infer\"'),\n",
      " (15,\n",
      "  '0.091*\"imag\" + 0.058*\"object\" + 0.035*\"train\" + 0.029*\"word\" + '\n",
      "  '0.022*\"translat\" + 0.019*\"task\" + 0.018*\"annot\" + 0.018*\"visual\" + '\n",
      "  '0.017*\"supervis\" + 0.015*\"discrimin\"'),\n",
      " (16,\n",
      "  '0.152*\"function\" + 0.092*\"loss\" + 0.079*\"regress\" + 0.035*\"gp\" + '\n",
      "  '0.033*\"submodular\" + 0.020*\"phase\" + 0.017*\"basi\" + 0.013*\"avoid\" + '\n",
      "  '0.011*\"logist\" + 0.011*\"residu\"'),\n",
      " (17,\n",
      "  '0.043*\"neuron\" + 0.036*\"spike\" + 0.032*\"mechan\" + 0.025*\"neural\" + '\n",
      "  '0.021*\"respons\" + 0.017*\"signal\" + 0.016*\"popul\" + 0.016*\"inject\" + '\n",
      "  '0.016*\"filter\" + 0.012*\"rule\"'),\n",
      " (18,\n",
      "  '0.065*\"metric\" + 0.056*\"topic\" + 0.049*\"binari\" + 0.047*\"hash\" + '\n",
      "  '0.031*\"distanc\" + 0.030*\"document\" + 0.027*\"text\" + 0.025*\"rank\" + '\n",
      "  '0.021*\"rdp\" + 0.017*\"energi\"'),\n",
      " (19,\n",
      "  '0.062*\"distribut\" + 0.054*\"latent\" + 0.049*\"densiti\" + 0.041*\"factor\" + '\n",
      "  '0.035*\"mixtur\" + 0.029*\"margin\" + 0.028*\"variabl\" + 0.024*\"compon\" + '\n",
      "  '0.019*\"kde\" + 0.018*\"nonparametr\"')]\n",
      "[(43,\n",
      "  '0.076*\"et\" + 0.076*\"al\" + 0.068*\"sampler\" + 0.042*\"on\" + 0.040*\"progress\" + '\n",
      "  '0.036*\"gibb\" + 0.030*\"denois\" + 0.017*\"lsh\" + 0.017*\"de\" + 0.015*\"fairli\"'),\n",
      " (5,\n",
      "  '0.141*\"topic\" + 0.077*\"document\" + 0.067*\"text\" + 0.037*\"fair\" + '\n",
      "  '0.032*\"coupl\" + 0.031*\"ad\" + 0.023*\"impos\" + 0.021*\"corpora\" + 0.020*\"ml\" + '\n",
      "  '0.018*\"vector\"'),\n",
      " (26,\n",
      "  '0.097*\"rule\" + 0.055*\"treewidth\" + 0.046*\"expert\" + 0.037*\"databas\" + '\n",
      "  '0.029*\"discount\" + 0.025*\"polytop\" + 0.025*\"option\" + 0.021*\"go\" + '\n",
      "  '0.019*\"sure\" + 0.017*\"multistep\"'),\n",
      " (41,\n",
      "  '0.123*\"neuron\" + 0.076*\"spike\" + 0.034*\"togeth\" + 0.031*\"synapt\" + '\n",
      "  '0.030*\"overlap\" + 0.028*\"waveform\" + 0.026*\"net\" + 0.024*\"ensembl\" + '\n",
      "  '0.024*\"necessari\" + 0.023*\"easi\"'),\n",
      " (15,\n",
      "  '0.234*\"cluster\" + 0.138*\"metric\" + 0.049*\"pairwis\" + 0.045*\"rdp\" + '\n",
      "  '0.027*\"intuit\" + 0.021*\"consensu\" + 0.018*\"mistak\" + 0.015*\"hope\" + '\n",
      "  '0.012*\"union\" + 0.010*\"oppos\"'),\n",
      " (47,\n",
      "  '0.102*\"rel\" + 0.065*\"rather\" + 0.050*\"grow\" + 0.032*\"reflect\" + '\n",
      "  '0.029*\"impact\" + 0.027*\"decreas\" + 0.024*\"examin\" + 0.024*\"channel\" + '\n",
      "  '0.021*\"pool\" + 0.020*\"degrad\"'),\n",
      " (13,\n",
      "  '0.117*\"polici\" + 0.100*\"control\" + 0.081*\"reward\" + 0.065*\"feedback\" + '\n",
      "  '0.058*\"prefer\" + 0.045*\"reinforc\" + 0.043*\"mdp\" + 0.028*\"valu\" + '\n",
      "  '0.024*\"expert\" + 0.011*\"plan\"'),\n",
      " (31,\n",
      "  '0.147*\"human\" + 0.072*\"filter\" + 0.047*\"cognit\" + 0.031*\"situat\" + '\n",
      "  '0.031*\"taxonomi\" + 0.028*\"composit\" + 0.025*\"ask\" + 0.021*\"capac\" + '\n",
      "  '0.021*\"move\" + 0.020*\"schedul\"'),\n",
      " (4,\n",
      "  '0.283*\"represent\" + 0.043*\"bay\" + 0.040*\"draw\" + 0.033*\"patch\" + '\n",
      "  '0.025*\"realiz\" + 0.023*\"throughout\" + 0.022*\"probe\" + 0.020*\"tend\" + '\n",
      "  '0.018*\"rfn\" + 0.017*\"pursuit\"'),\n",
      " (7,\n",
      "  '0.280*\"graph\" + 0.063*\"relax\" + 0.037*\"align\" + 0.025*\"monoton\" + '\n",
      "  '0.025*\"dbh\" + 0.020*\"sensor\" + 0.017*\"gene\" + 0.016*\"locationvari\" + '\n",
      "  '0.016*\"correspond\" + 0.015*\"vertic\"'),\n",
      " (39,\n",
      "  '0.093*\"measur\" + 0.046*\"maxim\" + 0.042*\"unknown\" + 0.041*\"similar\" + '\n",
      "  '0.038*\"match\" + 0.036*\"defin\" + 0.035*\"notion\" + 0.035*\"enabl\" + '\n",
      "  '0.033*\"respect\" + 0.029*\"finit\"'),\n",
      " (36,\n",
      "  '0.104*\"process\" + 0.072*\"bayesian\" + 0.071*\"state\" + 0.058*\"gaussian\" + '\n",
      "  '0.050*\"prior\" + 0.046*\"probabilist\" + 0.036*\"markov\" + 0.028*\"art\" + '\n",
      "  '0.026*\"nonlinear\" + 0.024*\"support\"'),\n",
      " (29,\n",
      "  '0.220*\"estim\" + 0.171*\"distribut\" + 0.157*\"sampl\" + 0.057*\"probabl\" + '\n",
      "  '0.046*\"test\" + 0.040*\"statist\" + 0.034*\"famili\" + 0.022*\"distanc\" + '\n",
      "  '0.022*\"suffici\" + 0.019*\"comparison\"'),\n",
      " (16,\n",
      "  '0.312*\"data\" + 0.062*\"prove\" + 0.059*\"weight\" + 0.042*\"real\" + '\n",
      "  '0.036*\"point\" + 0.031*\"synthet\" + 0.031*\"avail\" + 0.026*\"group\" + '\n",
      "  '0.022*\"assum\" + 0.019*\"simul\"'),\n",
      " (1,\n",
      "  '0.157*\"function\" + 0.130*\"optim\" + 0.070*\"converg\" + 0.063*\"rate\" + '\n",
      "  '0.056*\"gradient\" + 0.041*\"convex\" + 0.039*\"minim\" + 0.039*\"error\" + '\n",
      "  '0.029*\"descent\" + 0.027*\"method\"'),\n",
      " (40,\n",
      "  '0.070*\"stochast\" + 0.059*\"empir\" + 0.057*\"theoret\" + 0.041*\"adapt\" + '\n",
      "  '0.032*\"particular\" + 0.030*\"valu\" + 0.030*\"practic\" + 0.027*\"step\" + '\n",
      "  '0.027*\"separ\" + 0.027*\"theori\"'),\n",
      " (49,\n",
      "  '0.301*\"model\" + 0.074*\"structur\" + 0.064*\"infer\" + 0.040*\"variabl\" + '\n",
      "  '0.029*\"variat\" + 0.024*\"latent\" + 0.024*\"context\" + 0.019*\"captur\" + '\n",
      "  '0.018*\"posterior\" + 0.017*\"across\"'),\n",
      " (37,\n",
      "  '0.143*\"learn\" + 0.055*\"approach\" + 0.039*\"propos\" + 0.034*\"task\" + '\n",
      "  '0.031*\"predict\" + 0.030*\"featur\" + 0.027*\"use\" + 0.027*\"method\" + '\n",
      "  '0.025*\"differ\" + 0.024*\"dataset\"'),\n",
      " (10,\n",
      "  '0.088*\"algorithm\" + 0.028*\"studi\" + 0.026*\"comput\" + 0.024*\"problem\" + '\n",
      "  '0.024*\"show\" + 0.022*\"number\" + 0.021*\"approxim\" + 0.021*\"effici\" + '\n",
      "  '0.021*\"result\" + 0.019*\"provid\"'),\n",
      " (27,\n",
      "  '0.035*\"problem\" + 0.034*\"method\" + 0.031*\"gener\" + 0.027*\"use\" + '\n",
      "  '0.026*\"set\" + 0.022*\"show\" + 0.019*\"framework\" + 0.017*\"result\" + '\n",
      "  '0.014*\"complex\" + 0.014*\"class\"')]\n",
      "[(68,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (53,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (65,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (81,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (13,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (63,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (29,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (94,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (85,\n",
      "  '0.000*\"collisionfre\" + 0.000*\"tem\" + 0.000*\"ist\" + 0.000*\"helicopt\" + '\n",
      "  '0.000*\"drd\" + 0.000*\"partiallyknown\" + 0.000*\"me\" + 0.000*\"spe\" + '\n",
      "  '0.000*\"household\" + 0.000*\"sy\"'),\n",
      " (3,\n",
      "  '0.396*\"help\" + 0.000*\"drd\" + 0.000*\"motionplan\" + 0.000*\"ist\" + '\n",
      "  '0.000*\"helicopt\" + 0.000*\"partiallyknown\" + 0.000*\"collisionfre\" + '\n",
      "  '0.000*\"me\" + 0.000*\"spe\" + 0.000*\"household\"'),\n",
      " (32,\n",
      "  '0.318*\"network\" + 0.130*\"neural\" + 0.124*\"deep\" + 0.107*\"train\" + '\n",
      "  '0.066*\"convolut\" + 0.053*\"architectur\" + 0.040*\"recurr\" + 0.032*\"input\" + '\n",
      "  '0.021*\"capabl\" + 0.018*\"shape\"'),\n",
      " (58,\n",
      "  '0.214*\"stochast\" + 0.194*\"gradient\" + 0.141*\"local\" + 0.100*\"descent\" + '\n",
      "  '0.080*\"global\" + 0.060*\"updat\" + 0.046*\"nonconvex\" + 0.030*\"pose\" + '\n",
      "  '0.022*\"optim\" + 0.017*\"suboptim\"'),\n",
      " (51,\n",
      "  '0.124*\"scale\" + 0.107*\"within\" + 0.092*\"significantli\" + 0.083*\"increas\" + '\n",
      "  '0.073*\"version\" + 0.064*\"accur\" + 0.060*\"interpret\" + 0.058*\"origin\" + '\n",
      "  '0.051*\"becom\" + 0.047*\"typic\"'),\n",
      " (7,\n",
      "  '0.289*\"estim\" + 0.228*\"bound\" + 0.103*\"statist\" + 0.061*\"lower\" + '\n",
      "  '0.058*\"mean\" + 0.044*\"risk\" + 0.034*\"highdimension\" + 0.027*\"upper\" + '\n",
      "  '0.020*\"minimax\" + 0.017*\"complex\"'),\n",
      " (37,\n",
      "  '0.414*\"model\" + 0.117*\"structur\" + 0.089*\"infer\" + 0.071*\"paramet\" + '\n",
      "  '0.031*\"probabilist\" + 0.029*\"repres\" + 0.028*\"altern\" + 0.020*\"underli\" + '\n",
      "  '0.020*\"graphic\" + 0.019*\"simul\"'),\n",
      " (97,\n",
      "  '0.142*\"function\" + 0.045*\"effect\" + 0.042*\"prove\" + 0.040*\"larg\" + '\n",
      "  '0.035*\"multipl\" + 0.029*\"real\" + 0.026*\"defin\" + 0.026*\"problem\" + '\n",
      "  '0.024*\"maxim\" + 0.024*\"includ\"'),\n",
      " (77,\n",
      "  '0.313*\"algorithm\" + 0.090*\"problem\" + 0.073*\"set\" + 0.058*\"analysi\" + '\n",
      "  '0.046*\"theoret\" + 0.044*\"onlin\" + 0.043*\"guarante\" + 0.028*\"design\" + '\n",
      "  '0.027*\"provid\" + 0.023*\"size\"'),\n",
      " (10,\n",
      "  '0.061*\"comput\" + 0.051*\"effici\" + 0.051*\"number\" + 0.047*\"show\" + '\n",
      "  '0.046*\"also\" + 0.045*\"result\" + 0.045*\"time\" + 0.039*\"linear\" + '\n",
      "  '0.033*\"achiev\" + 0.028*\"make\"'),\n",
      " (27,\n",
      "  '0.073*\"optim\" + 0.053*\"studi\" + 0.047*\"gener\" + 0.038*\"class\" + '\n",
      "  '0.037*\"problem\" + 0.035*\"show\" + 0.032*\"observ\" + 0.031*\"condit\" + '\n",
      "  '0.026*\"case\" + 0.025*\"consid\"'),\n",
      " (64,\n",
      "  '0.079*\"learn\" + 0.060*\"method\" + 0.056*\"use\" + 0.052*\"data\" + '\n",
      "  '0.044*\"propos\" + 0.032*\"approach\" + 0.026*\"perform\" + 0.022*\"base\" + '\n",
      "  '0.021*\"new\" + 0.021*\"present\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model5.print_topics())\n",
    "doc_lda = lda_model5[corpus]\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model10.print_topics())\n",
    "doc_lda = lda_model10[corpus]\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model20.print_topics())\n",
    "doc_lda = lda_model20[corpus]\n",
    "\n",
    "# Print the Keyword in the 50 topics\n",
    "pprint(lda_model50.print_topics())\n",
    "doc_lda = lda_model50[corpus]\n",
    "\n",
    "# Print the Keyword in the 100 topics\n",
    "pprint(lda_model100.print_topics())\n",
    "doc_lda = lda_model100[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.3370528837678055\n",
      "\n",
      "Perplexity5:  -7.311895204650633\n",
      "\n",
      "Perplexity10:  -7.372718990378462\n",
      "\n",
      "Perplexity20:  -7.3834574983049714\n",
      "\n",
      "Perplexity50:  -7.420792239026178\n",
      "\n",
      "Perplexity100:  -21.60733735376125\n",
      "\n",
      "Coherence Score:  0.263846120041168\n",
      "\n",
      "Coherence Score5:  0.40416796057366317\n",
      "\n",
      "Coherence Score10:  0.41489065613483633\n",
      "\n",
      "Coherence Score20:  0.3985465790172792\n",
      "\n",
      "Coherence Score50:  0.4112285725887475\n",
      "\n",
      "Coherence Score100:  0.4135019669962473\n",
      "\n",
      "Coherence ScoreUMass:  -1.1975019153673416\n",
      "\n",
      "Coherence Score5UMass:  -2.035671328726481\n",
      "\n",
      "Coherence Score10UMass:  -5.282097582661903\n",
      "\n",
      "Coherence Score20UMass:  -6.46925754585322\n",
      "\n",
      "Coherence Score50UMass:  -9.395029709372729\n",
      "\n",
      "Coherence Score100UMass:  -12.071492459401174\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity5: ', lda_model5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity10: ', lda_model10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity20: ', lda_model20.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity50: ', lda_model50.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity100: ', lda_model100.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100: ', coherence_lda)\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence ScoreUMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100UMass: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
