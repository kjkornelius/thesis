{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inlin\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/Kornelius/Desktop/Data 2/nips-papers/papers.csv', header = 0, sep = ';', error_bad_lines=False)\n",
    "# Drop the columns not needed\n",
    "df = df.drop(columns=['id', 'event_type', 'pdf_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row by condition\n",
    "df = df[df.abstract != 'Abstract Missing']\n",
    "# Print out the first rows of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.abstract.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(data):\n",
    "    stop_free = \" \".join([i for i in data.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data = [clean(data).split() for data in data]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove characters and numbers\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16129 unique tokens: ['activity', 'also', 'analysis', 'applied', 'assume']...)\n"
     ]
    }
   ],
   "source": [
    "data = data_words\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data\n",
    "# less than 10 documents,\n",
    "id2word = corpora.Dictionary(texts)\n",
    "id2word.filter_extremes(no_below = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data\n",
    "# Term Document Frequency and creating corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model20 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model50 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model100 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=100, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.015*\"algorithm\" + 0.015*\"model\" + 0.011*\"learning\" + 0.011*\"problem\" + '\n",
      "  '0.011*\"method\" + 0.010*\"data\" + 0.008*\"show\" + 0.007*\"result\" + '\n",
      "  '0.006*\"approach\" + 0.006*\"function\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 1 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.055*\"model\" + 0.052*\"network\" + 0.026*\"neural\" + 0.012*\"neuron\" + '\n",
      "  '0.012*\"system\" + 0.011*\"memory\" + 0.011*\"deep\" + 0.010*\"human\" + '\n",
      "  '0.009*\"convolutional\" + 0.009*\"pattern\"'),\n",
      " (1,\n",
      "  '0.024*\"algorithm\" + 0.024*\"learning\" + 0.015*\"problem\" + 0.014*\"online\" + '\n",
      "  '0.013*\"regret\" + 0.012*\"study\" + 0.011*\"setting\" + 0.011*\"bound\" + '\n",
      "  '0.010*\"time\" + 0.009*\"show\"'),\n",
      " (2,\n",
      "  '0.022*\"data\" + 0.020*\"learning\" + 0.018*\"model\" + 0.016*\"image\" + '\n",
      "  '0.015*\"task\" + 0.014*\"method\" + 0.013*\"feature\" + 0.012*\"approach\" + '\n",
      "  '0.011*\"label\" + 0.010*\"training\"'),\n",
      " (3,\n",
      "  '0.043*\"algorithm\" + 0.033*\"problem\" + 0.031*\"method\" + 0.021*\"function\" + '\n",
      "  '0.013*\"approach\" + 0.012*\"optimization\" + 0.011*\"learning\" + 0.010*\"show\" + '\n",
      "  '0.010*\"stochastic\" + 0.010*\"inference\"'),\n",
      " (4,\n",
      "  '0.019*\"model\" + 0.015*\"distribution\" + 0.015*\"data\" + 0.012*\"matrix\" + '\n",
      "  '0.010*\"result\" + 0.010*\"sample\" + 0.010*\"bound\" + 0.010*\"analysis\" + '\n",
      "  '0.009*\"graph\" + 0.009*\"show\"')]\n",
      "[(0,\n",
      "  '0.100*\"matrix\" + 0.057*\"sparse\" + 0.039*\"policy\" + 0.036*\"signal\" + '\n",
      "  '0.031*\"norm\" + 0.030*\"component\" + 0.027*\"reward\" + 0.025*\"robust\" + '\n",
      "  '0.021*\"group\" + 0.020*\"sparsity\"'),\n",
      " (1,\n",
      "  '0.041*\"learning\" + 0.032*\"label\" + 0.028*\"algorithm\" + 0.026*\"online\" + '\n",
      "  '0.025*\"loss\" + 0.025*\"regret\" + 0.024*\"problem\" + 0.023*\"class\" + '\n",
      "  '0.022*\"function\" + 0.020*\"setting\"'),\n",
      " (2,\n",
      "  '0.091*\"algorithm\" + 0.052*\"problem\" + 0.023*\"number\" + 0.020*\"graph\" + '\n",
      "  '0.015*\"time\" + 0.013*\"set\" + 0.013*\"linear\" + 0.013*\"convex\" + '\n",
      "  '0.012*\"approximation\" + 0.011*\"solution\"'),\n",
      " (3,\n",
      "  '0.042*\"method\" + 0.033*\"learning\" + 0.032*\"data\" + 0.026*\"approach\" + '\n",
      "  '0.018*\"propose\" + 0.014*\"framework\" + 0.013*\"show\" + 0.012*\"new\" + '\n",
      "  '0.012*\"performance\" + 0.012*\"demonstrate\"'),\n",
      " (4,\n",
      "  '0.021*\"result\" + 0.020*\"bound\" + 0.016*\"sample\" + 0.015*\"show\" + '\n",
      "  '0.014*\"analysis\" + 0.012*\"study\" + 0.012*\"provide\" + 0.012*\"error\" + '\n",
      "  '0.012*\"distribution\" + 0.011*\"estimation\"'),\n",
      " (5,\n",
      "  '0.038*\"optimization\" + 0.036*\"stochastic\" + 0.033*\"gradient\" + 0.031*\"rate\" '\n",
      "  '+ 0.030*\"convergence\" + 0.025*\"function\" + 0.017*\"descent\" + 0.015*\"method\" '\n",
      "  '+ 0.012*\"local\" + 0.011*\"game\"'),\n",
      " (6,\n",
      "  '0.047*\"task\" + 0.041*\"word\" + 0.040*\"context\" + 0.035*\"memory\" + '\n",
      "  '0.031*\"decision\" + 0.023*\"translation\" + 0.023*\"human\" + 0.023*\"sequence\" + '\n",
      "  '0.021*\"agent\" + 0.021*\"model\"'),\n",
      " (7,\n",
      "  '0.111*\"network\" + 0.093*\"image\" + 0.043*\"deep\" + 0.036*\"neural\" + '\n",
      "  '0.031*\"training\" + 0.020*\"convolutional\" + 0.018*\"architecture\" + '\n",
      "  '0.018*\"layer\" + 0.017*\"representation\" + 0.017*\"feature\"'),\n",
      " (8,\n",
      "  '0.146*\"model\" + 0.040*\"inference\" + 0.031*\"distribution\" + 0.029*\"variable\" '\n",
      "  '+ 0.028*\"process\" + 0.027*\"bayesian\" + 0.026*\"kernel\" + 0.019*\"structure\" + '\n",
      "  '0.019*\"gaussian\" + 0.019*\"latent\"'),\n",
      " (9,\n",
      "  '0.030*\"model\" + 0.015*\"state\" + 0.015*\"system\" + 0.014*\"information\" + '\n",
      "  '0.012*\"different\" + 0.010*\"using\" + 0.010*\"dynamic\" + 0.009*\"object\" + '\n",
      "  '0.009*\"action\" + 0.008*\"neuron\"')]\n",
      "[(0,\n",
      "  '0.120*\"sparse\" + 0.102*\"signal\" + 0.083*\"policy\" + 0.044*\"feedback\" + '\n",
      "  '0.043*\"measurement\" + 0.041*\"sparsity\" + 0.041*\"manifold\" + 0.032*\"group\" + '\n",
      "  '0.029*\"reinforcement\" + 0.019*\"synaptic\"'),\n",
      " (1,\n",
      "  '0.074*\"method\" + 0.067*\"data\" + 0.050*\"approach\" + 0.028*\"using\" + '\n",
      "  '0.027*\"propose\" + 0.023*\"proposed\" + 0.019*\"demonstrate\" + 0.018*\"set\" + '\n",
      "  '0.018*\"novel\" + 0.018*\"large\"'),\n",
      " (2,\n",
      "  '0.171*\"graph\" + 0.081*\"clustering\" + 0.058*\"tree\" + 0.051*\"cluster\" + '\n",
      "  '0.037*\"similarity\" + 0.034*\"working\" + 0.026*\"become\" + 0.023*\"edge\" + '\n",
      "  '0.021*\"path\" + 0.021*\"kmeans\"'),\n",
      " (3,\n",
      "  '0.140*\"learning\" + 0.063*\"label\" + 0.049*\"loss\" + 0.045*\"classification\" + '\n",
      "  '0.032*\"robust\" + 0.027*\"instance\" + 0.021*\"classifier\" + 0.019*\"example\" + '\n",
      "  '0.018*\"iteration\" + 0.018*\"active\"'),\n",
      " (4,\n",
      "  '0.034*\"framework\" + 0.023*\"analysis\" + 0.021*\"result\" + 0.020*\"complexity\" '\n",
      "  '+ 0.015*\"data\" + 0.015*\"show\" + 0.013*\"regularization\" + '\n",
      "  '0.013*\"statistical\" + 0.012*\"property\" + 0.012*\"noise\"'),\n",
      " (5,\n",
      "  '0.208*\"network\" + 0.102*\"neural\" + 0.048*\"neuron\" + 0.033*\"architecture\" + '\n",
      "  '0.023*\"alternating\" + 0.022*\"recurrent\" + 0.017*\"input\" + '\n",
      "  '0.017*\"connection\" + 0.014*\"hundred\" + 0.014*\"divergence\"'),\n",
      " (6,\n",
      "  '0.036*\"decision\" + 0.031*\"unknown\" + 0.028*\"reward\" + 0.025*\"making\" + '\n",
      "  '0.024*\"agent\" + 0.024*\"game\" + 0.023*\"human\" + 0.022*\"finite\" + '\n",
      "  '0.021*\"behavior\" + 0.018*\"bounded\"'),\n",
      " (7,\n",
      "  '0.168*\"bound\" + 0.076*\"regret\" + 0.050*\"lower\" + 0.045*\"metric\" + '\n",
      "  '0.042*\"bandit\" + 0.041*\"measure\" + 0.036*\"risk\" + 0.023*\"distance\" + '\n",
      "  '0.023*\"arm\" + 0.020*\"learner\"'),\n",
      " (8,\n",
      "  '0.075*\"code\" + 0.068*\"source\" + 0.041*\"separation\" + 0.039*\"coefficient\" + '\n",
      "  '0.038*\"matching\" + 0.033*\"reconstruction\" + 0.033*\"combinatorial\" + '\n",
      "  '0.029*\"speech\" + 0.027*\"usage\" + 0.027*\"geometric\"'),\n",
      " (9,\n",
      "  '0.060*\"time\" + 0.038*\"state\" + 0.035*\"system\" + 0.022*\"dynamic\" + '\n",
      "  '0.021*\"action\" + 0.017*\"control\" + 0.014*\"domain\" + 0.014*\"knowledge\" + '\n",
      "  '0.013*\"continuous\" + 0.013*\"gp\"'),\n",
      " (10,\n",
      "  '0.079*\"algorithm\" + 0.060*\"problem\" + 0.031*\"function\" + 0.020*\"show\" + '\n",
      "  '0.018*\"result\" + 0.017*\"learning\" + 0.015*\"optimization\" + 0.014*\"new\" + '\n",
      "  '0.014*\"method\" + 0.013*\"stochastic\"'),\n",
      " (11,\n",
      "  '0.111*\"task\" + 0.092*\"feature\" + 0.072*\"training\" + 0.055*\"learning\" + '\n",
      "  '0.051*\"deep\" + 0.031*\"weight\" + 0.024*\"convolutional\" + 0.024*\"learn\" + '\n",
      "  '0.021*\"art\" + 0.021*\"layer\"'),\n",
      " (12,\n",
      "  '0.081*\"word\" + 0.075*\"vector\" + 0.055*\"topic\" + 0.032*\"hashing\" + '\n",
      "  '0.031*\"document\" + 0.027*\"tensor\" + 0.027*\"language\" + 0.027*\"text\" + '\n",
      "  '0.027*\"lowrank\" + 0.024*\"in\"'),\n",
      " (13,\n",
      "  '0.082*\"matrix\" + 0.068*\"sample\" + 0.054*\"distribution\" + 0.044*\"estimator\" '\n",
      "  '+ 0.040*\"probability\" + 0.037*\"estimate\" + 0.037*\"sampling\" + '\n",
      "  '0.029*\"estimation\" + 0.025*\"test\" + 0.025*\"rank\"'),\n",
      " (14,\n",
      "  '0.034*\"number\" + 0.026*\"class\" + 0.024*\"provide\" + 0.022*\"parameter\" + '\n",
      "  '0.019*\"approximation\" + 0.018*\"regression\" + 0.016*\"family\" + '\n",
      "  '0.016*\"setting\" + 0.015*\"error\" + 0.014*\"linear\"'),\n",
      " (15,\n",
      "  '0.071*\"exploration\" + 0.052*\"applies\" + 0.048*\"svm\" + 0.043*\"encoding\" + '\n",
      "  '0.033*\"separate\" + 0.033*\"measured\" + 0.030*\"conditioning\" + 0.029*\"with\" + '\n",
      "  '0.026*\"mathematical\" + 0.025*\"cue\"'),\n",
      " (16,\n",
      "  '0.101*\"model\" + 0.069*\"inference\" + 0.052*\"process\" + 0.050*\"distribution\" '\n",
      "  '+ 0.050*\"variable\" + 0.046*\"bayesian\" + 0.035*\"gaussian\" + 0.033*\"latent\" + '\n",
      "  '0.028*\"probabilistic\" + 0.027*\"prior\"'),\n",
      " (17,\n",
      "  '0.212*\"image\" + 0.087*\"object\" + 0.046*\"visual\" + 0.038*\"generative\" + '\n",
      "  '0.033*\"preference\" + 0.027*\"video\" + 0.024*\"human\" + 0.021*\"motion\" + '\n",
      "  '0.018*\"transfer\" + 0.018*\"annotation\"'),\n",
      " (18,\n",
      "  '0.057*\"target\" + 0.057*\"mechanism\" + 0.046*\"spike\" + 0.045*\"rule\" + '\n",
      "  '0.037*\"difficult\" + 0.026*\"market\" + 0.024*\"entropy\" + 0.022*\"mapping\" + '\n",
      "  '0.021*\"spiking\" + 0.021*\"enough\"'),\n",
      " (19,\n",
      "  '0.094*\"model\" + 0.024*\"information\" + 0.018*\"structure\" + '\n",
      "  '0.016*\"prediction\" + 0.014*\"use\" + 0.013*\"different\" + 0.012*\"context\" + '\n",
      "  '0.012*\"representation\" + 0.012*\"memory\" + 0.011*\"study\"')]\n",
      "[(39,\n",
      "  '0.323*\"kernel\" + 0.202*\"embeddings\" + 0.080*\"completion\" + '\n",
      "  '0.052*\"threshold\" + 0.031*\"hilbert\" + 0.030*\"space\" + 0.030*\"impact\" + '\n",
      "  '0.025*\"frequently\" + 0.022*\"necessarily\" + 0.021*\"ridge\"'),\n",
      " (41,\n",
      "  '0.302*\"signal\" + 0.197*\"weight\" + 0.053*\"feedback\" + 0.040*\"play\" + '\n",
      "  '0.038*\"sensing\" + 0.032*\"role\" + 0.030*\"semidefinite\" + 0.028*\"rating\" + '\n",
      "  '0.022*\"trial\" + 0.022*\"collaborative\"'),\n",
      " (8,\n",
      "  '0.133*\"code\" + 0.129*\"pattern\" + 0.121*\"source\" + 0.072*\"separation\" + '\n",
      "  '0.061*\"especially\" + 0.055*\"found\" + 0.052*\"speech\" + 0.037*\"consists\" + '\n",
      "  '0.035*\"investigated\" + 0.034*\"poisson\"'),\n",
      " (14,\n",
      "  '0.144*\"markov\" + 0.131*\"mixture\" + 0.080*\"chain\" + 0.056*\"monte\" + '\n",
      "  '0.056*\"carlo\" + 0.053*\"gamma\" + 0.051*\"mcmc\" + 0.042*\"dirichlet\" + '\n",
      "  '0.033*\"allocation\" + 0.028*\"process\"'),\n",
      " (32,\n",
      "  '0.179*\"rank\" + 0.104*\"semisupervised\" + 0.088*\"adversarial\" + '\n",
      "  '0.060*\"variation\" + 0.045*\"introduces\" + 0.044*\"tackle\" + 0.037*\"endtoend\" '\n",
      "  '+ 0.033*\"ground\" + 0.030*\"truth\" + 0.029*\"generative\"'),\n",
      " (37,\n",
      "  '0.108*\"neuron\" + 0.066*\"spike\" + 0.063*\"neural\" + 0.055*\"response\" + '\n",
      "  '0.053*\"functional\" + 0.051*\"connectivity\" + 0.047*\"activity\" + '\n",
      "  '0.042*\"stimulus\" + 0.042*\"filter\" + 0.031*\"synaptic\"'),\n",
      " (7,\n",
      "  '0.124*\"parallel\" + 0.096*\"unit\" + 0.090*\"original\" + 0.069*\"matching\" + '\n",
      "  '0.059*\"hard\" + 0.055*\"speedup\" + 0.041*\"asynchronous\" + 0.040*\"alpha\" + '\n",
      "  '0.040*\"inherent\" + 0.039*\"iteratively\"'),\n",
      " (46,\n",
      "  '0.152*\"translation\" + 0.096*\"support\" + 0.046*\"svm\" + 0.044*\"efficacy\" + '\n",
      "  '0.041*\"location\" + 0.041*\"geometry\" + 0.041*\"depth\" + 0.033*\"mrf\" + '\n",
      "  '0.032*\"designed\" + 0.032*\"directed\"'),\n",
      " (26,\n",
      "  '0.378*\"graph\" + 0.170*\"measure\" + 0.082*\"similarity\" + 0.051*\"edge\" + '\n",
      "  '0.038*\"combined\" + 0.037*\"neighbor\" + 0.023*\"gene\" + 0.021*\"expression\" + '\n",
      "  '0.019*\"expressed\" + 0.018*\"ordinary\"'),\n",
      " (21,\n",
      "  '0.098*\"manifold\" + 0.078*\"em\" + 0.050*\"come\" + 0.050*\"applying\" + '\n",
      "  '0.045*\"riemannian\" + 0.045*\"party\" + 0.041*\"worker\" + 0.038*\"protocol\" + '\n",
      "  '0.035*\"complete\" + 0.034*\"alignment\"'),\n",
      " (9,\n",
      "  '0.053*\"decision\" + 0.048*\"probability\" + 0.040*\"knowledge\" + 0.040*\"making\" '\n",
      "  '+ 0.037*\"version\" + 0.035*\"conditional\" + 0.030*\"common\" + 0.028*\"three\" + '\n",
      "  '0.027*\"finding\" + 0.027*\"pair\"'),\n",
      " (33,\n",
      "  '0.199*\"function\" + 0.095*\"optimization\" + 0.081*\"rate\" + '\n",
      "  '0.071*\"convergence\" + 0.059*\"convex\" + 0.041*\"problem\" + 0.033*\"risk\" + '\n",
      "  '0.029*\"minimization\" + 0.028*\"achieves\" + 0.021*\"score\"'),\n",
      " (17,\n",
      "  '0.514*\"model\" + 0.073*\"process\" + 0.046*\"latent\" + 0.035*\"structure\" + '\n",
      "  '0.033*\"posterior\" + 0.027*\"modeling\" + 0.026*\"capture\" + 0.023*\"generative\" '\n",
      "  '+ 0.016*\"temporal\" + 0.016*\"ability\"'),\n",
      " (44,\n",
      "  '0.363*\"learning\" + 0.123*\"task\" + 0.063*\"machine\" + 0.063*\"learn\" + '\n",
      "  '0.051*\"representation\" + 0.031*\"domain\" + 0.028*\"eg\" + 0.028*\"benchmark\" + '\n",
      "  '0.026*\"learned\" + 0.018*\"useful\"'),\n",
      " (19,\n",
      "  '0.296*\"data\" + 0.058*\"system\" + 0.037*\"dynamic\" + 0.036*\"scale\" + '\n",
      "  '0.027*\"across\" + 0.026*\"apply\" + 0.023*\"brain\" + 0.022*\"reduction\" + '\n",
      "  '0.018*\"distributed\" + 0.018*\"need\"'),\n",
      " (13,\n",
      "  '0.083*\"bound\" + 0.057*\"sample\" + 0.042*\"complexity\" + 0.042*\"error\" + '\n",
      "  '0.040*\"estimation\" + 0.037*\"random\" + 0.035*\"estimator\" + 0.032*\"parameter\" '\n",
      "  '+ 0.031*\"statistical\" + 0.030*\"estimate\"'),\n",
      " (0,\n",
      "  '0.029*\"number\" + 0.020*\"small\" + 0.019*\"strategy\" + 0.014*\"solving\" + '\n",
      "  '0.011*\"classical\" + 0.011*\"expert\" + 0.010*\"individual\" + 0.010*\"close\" + '\n",
      "  '0.009*\"efficiency\" + 0.009*\"alternating\"'),\n",
      " (47,\n",
      "  '0.266*\"algorithm\" + 0.116*\"problem\" + 0.054*\"time\" + 0.038*\"optimal\" + '\n",
      "  '0.033*\"number\" + 0.032*\"guarantee\" + 0.031*\"approximation\" + 0.023*\"order\" '\n",
      "  '+ 0.023*\"sampling\" + 0.023*\"generalization\"'),\n",
      " (28,\n",
      "  '0.051*\"study\" + 0.043*\"framework\" + 0.037*\"class\" + 0.034*\"analysis\" + '\n",
      "  '0.033*\"setting\" + 0.033*\"result\" + 0.027*\"general\" + 0.025*\"case\" + '\n",
      "  '0.025*\"provide\" + 0.023*\"structure\"'),\n",
      " (30,\n",
      "  '0.047*\"method\" + 0.035*\"show\" + 0.025*\"approach\" + 0.020*\"propose\" + '\n",
      "  '0.020*\"using\" + 0.017*\"new\" + 0.016*\"based\" + 0.016*\"result\" + '\n",
      "  '0.015*\"present\" + 0.015*\"set\"')]\n",
      "[(86,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (87,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (21,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (89,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (16,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (26,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (17,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (67,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (5,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (74,\n",
      "  '0.000*\"facial\" + 0.000*\"it\" + 0.000*\"em\" + 0.000*\"generally\" + '\n",
      "  '0.000*\"priori\" + 0.000*\"profile\" + 0.000*\"redundant\" + 0.000*\"robotic\" + '\n",
      "  '0.000*\"tackling\" + 0.000*\"extends\"'),\n",
      " (88,\n",
      "  '0.277*\"time\" + 0.110*\"order\" + 0.083*\"give\" + 0.065*\"faster\" + '\n",
      "  '0.059*\"fixed\" + 0.057*\"contrast\" + 0.046*\"learner\" + 0.036*\"computing\" + '\n",
      "  '0.032*\"magnitude\" + 0.025*\"main\"'),\n",
      " (2,\n",
      "  '0.185*\"optimal\" + 0.129*\"given\" + 0.122*\"solution\" + 0.056*\"independent\" + '\n",
      "  '0.043*\"that\" + 0.043*\"constant\" + 0.037*\"popular\" + 0.035*\"in\" + '\n",
      "  '0.033*\"belief\" + 0.027*\"market\"'),\n",
      " (93,\n",
      "  '0.226*\"bound\" + 0.143*\"sample\" + 0.102*\"estimation\" + 0.088*\"estimator\" + '\n",
      "  '0.082*\"prove\" + 0.067*\"lower\" + 0.060*\"mean\" + 0.048*\"risk\" + '\n",
      "  '0.033*\"statistical\" + 0.032*\"estimating\"'),\n",
      " (39,\n",
      "  '0.245*\"number\" + 0.142*\"large\" + 0.087*\"high\" + 0.077*\"small\" + '\n",
      "  '0.076*\"scale\" + 0.070*\"size\" + 0.036*\"dataset\" + 0.031*\"dimensional\" + '\n",
      "  '0.030*\"yet\" + 0.028*\"amount\"'),\n",
      " (40,\n",
      "  '0.711*\"model\" + 0.083*\"learn\" + 0.041*\"nonparametric\" + 0.026*\"highly\" + '\n",
      "  '0.020*\"effectively\" + 0.020*\"interaction\" + 0.019*\"train\" + '\n",
      "  '0.017*\"conditioned\" + 0.016*\"predict\" + 0.015*\"explicitly\"'),\n",
      " (34,\n",
      "  '0.149*\"study\" + 0.110*\"structure\" + 0.051*\"different\" + 0.037*\"set\" + '\n",
      "  '0.034*\"find\" + 0.033*\"unknown\" + 0.030*\"across\" + 0.029*\"possible\" + '\n",
      "  '0.027*\"node\" + 0.026*\"complex\"'),\n",
      " (60,\n",
      "  '0.229*\"data\" + 0.069*\"linear\" + 0.034*\"observation\" + 0.031*\"real\" + '\n",
      "  '0.028*\"stateoftheart\" + 0.026*\"allows\" + 0.025*\"via\" + 0.025*\"form\" + '\n",
      "  '0.023*\"eg\" + 0.023*\"using\"'),\n",
      " (28,\n",
      "  '0.119*\"problem\" + 0.080*\"framework\" + 0.059*\"analysis\" + 0.053*\"general\" + '\n",
      "  '0.046*\"case\" + 0.033*\"term\" + 0.023*\"derive\" + 0.022*\"design\" + '\n",
      "  '0.022*\"apply\" + 0.020*\"analyze\"'),\n",
      " (47,\n",
      "  '0.129*\"learning\" + 0.074*\"approach\" + 0.050*\"propose\" + 0.046*\"performance\" '\n",
      "  '+ 0.041*\"proposed\" + 0.041*\"novel\" + 0.030*\"several\" + 0.029*\"space\" + '\n",
      "  '0.029*\"demonstrate\" + 0.026*\"machine\"'),\n",
      " (30,\n",
      "  '0.091*\"algorithm\" + 0.058*\"method\" + 0.046*\"show\" + 0.035*\"result\" + '\n",
      "  '0.033*\"problem\" + 0.022*\"using\" + 0.022*\"based\" + 0.021*\"present\" + '\n",
      "  '0.021*\"also\" + 0.019*\"two\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model5.print_topics())\n",
    "doc_lda = lda_model5[corpus]\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model10.print_topics())\n",
    "doc_lda = lda_model10[corpus]\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model20.print_topics())\n",
    "doc_lda = lda_model20[corpus]\n",
    "\n",
    "# Print the Keyword in the 50 topics\n",
    "pprint(lda_model50.print_topics())\n",
    "doc_lda = lda_model50[corpus]\n",
    "\n",
    "# Print the Keyword in the 100 topics\n",
    "pprint(lda_model100.print_topics())\n",
    "doc_lda = lda_model100[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.141634942078457\n",
      "\n",
      "Perplexity5:  -7.074515027500307\n",
      "\n",
      "Perplexity10:  -7.084844565504205\n",
      "\n",
      "Perplexity20:  -7.0840042105178\n",
      "\n",
      "Perplexity50:  -7.054904438093043\n",
      "\n",
      "Perplexity100:  -18.260099025758166\n",
      "\n",
      "Coherence Score:  0.2357683522987768\n",
      "\n",
      "Coherence Score5:  0.42141946621857673\n",
      "\n",
      "Coherence Score10:  0.3998864985796514\n",
      "\n",
      "Coherence Score20:  0.39464949528233373\n",
      "\n",
      "Coherence Score50:  0.3813321304434668\n",
      "\n",
      "Coherence Score100:  0.4109786328060066\n",
      "\n",
      "Coherence ScoreUMass:  -1.4635242209070833\n",
      "\n",
      "Coherence Score5UMass:  -1.9889367592225617\n",
      "\n",
      "Coherence Score10UMass:  -2.6230649866647338\n",
      "\n",
      "Coherence Score20UMass:  -5.725642045561722\n",
      "\n",
      "Coherence Score50UMass:  -8.333768014296751\n",
      "\n",
      "Coherence Score100UMass:  -9.330269234163294\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity5: ', lda_model5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity10: ', lda_model10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity20: ', lda_model20.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity50: ', lda_model50.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity100: ', lda_model100.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100: ', coherence_lda)\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence ScoreUMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100UMass: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
