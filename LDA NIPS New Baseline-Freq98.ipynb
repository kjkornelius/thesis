{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inlin\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/Kornelius/Desktop/Data 2/nips-papers/papers.csv', header = 0, sep = ';', error_bad_lines=False)\n",
    "# Drop the columns not needed\n",
    "df = df.drop(columns=['id', 'event_type', 'pdf_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row by condition\n",
    "df = df[df.abstract != 'Abstract Missing']\n",
    "# Print out the first rows of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.abstract.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(data):\n",
    "    stop_free = \" \".join([i for i in data.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data = [clean(data).split() for data in data]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove characters and numbers\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16129 unique tokens: ['activity', 'also', 'analysis', 'applied', 'assume']...)\n"
     ]
    }
   ],
   "source": [
    "data = data_words\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4415 unique tokens: ['activity', 'also', 'analysis', 'applied', 'assume']...)\n"
     ]
    }
   ],
   "source": [
    "# filter words that occur in more than 90% of documents\n",
    "id2word.filter_extremes(no_above = 0.980)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data\n",
    "# Term Document Frequency and creating corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model20 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model50 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model100 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=100, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"algorithm\" + 0.014*\"model\" + 0.011*\"learning\" + 0.011*\"problem\" + '\n",
      "  '0.011*\"method\" + 0.009*\"data\" + 0.008*\"show\" + 0.006*\"result\" + '\n",
      "  '0.006*\"approach\" + 0.006*\"function\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 1 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.026*\"learning\" + 0.024*\"data\" + 0.023*\"method\" + 0.019*\"approach\" + '\n",
      "  '0.014*\"distribution\" + 0.010*\"model\" + 0.009*\"framework\" + 0.009*\"label\" + '\n",
      "  '0.009*\"propose\" + 0.008*\"task\"'),\n",
      " (1,\n",
      "  '0.026*\"learning\" + 0.023*\"algorithm\" + 0.017*\"online\" + 0.016*\"regret\" + '\n",
      "  '0.015*\"problem\" + 0.015*\"bound\" + 0.013*\"setting\" + 0.012*\"study\" + '\n",
      "  '0.011*\"optimal\" + 0.010*\"policy\"'),\n",
      " (2,\n",
      "  '0.050*\"image\" + 0.022*\"network\" + 0.022*\"deep\" + 0.020*\"object\" + '\n",
      "  '0.019*\"word\" + 0.016*\"task\" + 0.015*\"training\" + 0.012*\"representation\" + '\n",
      "  '0.012*\"neural\" + 0.011*\"translation\"'),\n",
      " (3,\n",
      "  '0.062*\"model\" + 0.015*\"network\" + 0.013*\"structure\" + 0.011*\"data\" + '\n",
      "  '0.010*\"system\" + 0.008*\"signal\" + 0.008*\"inference\" + 0.007*\"memory\" + '\n",
      "  '0.007*\"variable\" + 0.007*\"information\"'),\n",
      " (4,\n",
      "  '0.031*\"algorithm\" + 0.022*\"problem\" + 0.013*\"function\" + 0.011*\"result\" + '\n",
      "  '0.011*\"method\" + 0.010*\"show\" + 0.009*\"matrix\" + 0.008*\"optimization\" + '\n",
      "  '0.008*\"bound\" + 0.007*\"graph\"')]\n",
      "[(0,\n",
      "  '0.067*\"learning\" + 0.026*\"data\" + 0.025*\"approach\" + 0.023*\"task\" + '\n",
      "  '0.017*\"label\" + 0.016*\"feature\" + 0.014*\"method\" + 0.014*\"propose\" + '\n",
      "  '0.013*\"framework\" + 0.013*\"classification\"'),\n",
      " (1,\n",
      "  '0.067*\"state\" + 0.042*\"policy\" + 0.038*\"decision\" + 0.029*\"reward\" + '\n",
      "  '0.025*\"control\" + 0.024*\"system\" + 0.022*\"gp\" + 0.019*\"dynamic\" + '\n",
      "  '0.019*\"process\" + 0.017*\"trajectory\"'),\n",
      " (2,\n",
      "  '0.094*\"image\" + 0.039*\"object\" + 0.037*\"word\" + 0.024*\"representation\" + '\n",
      "  '0.019*\"visual\" + 0.017*\"human\" + 0.014*\"recognition\" + 0.013*\"feature\" + '\n",
      "  '0.013*\"hierarchical\" + 0.012*\"code\"'),\n",
      " (3,\n",
      "  '0.102*\"model\" + 0.019*\"inference\" + 0.014*\"structure\" + 0.011*\"information\" '\n",
      "  '+ 0.009*\"latent\" + 0.008*\"show\" + 0.008*\"system\" + 0.008*\"using\" + '\n",
      "  '0.007*\"topic\" + 0.007*\"neuron\"'),\n",
      " (4,\n",
      "  '0.027*\"matrix\" + 0.022*\"graph\" + 0.015*\"random\" + 0.015*\"estimator\" + '\n",
      "  '0.013*\"estimation\" + 0.012*\"probability\" + 0.011*\"distribution\" + '\n",
      "  '0.011*\"structure\" + 0.011*\"prove\" + 0.010*\"regression\"'),\n",
      " (5,\n",
      "  '0.048*\"algorithm\" + 0.024*\"bound\" + 0.019*\"result\" + 0.018*\"study\" + '\n",
      "  '0.016*\"setting\" + 0.016*\"number\" + 0.014*\"show\" + 0.012*\"time\" + '\n",
      "  '0.011*\"online\" + 0.011*\"complexity\"'),\n",
      " (6,\n",
      "  '0.045*\"action\" + 0.036*\"tree\" + 0.031*\"agent\" + 0.029*\"active\" + '\n",
      "  '0.026*\"query\" + 0.023*\"expert\" + 0.020*\"strategy\" + 0.018*\"player\" + '\n",
      "  '0.016*\"annotator\" + 0.015*\"exploration\"'),\n",
      " (7,\n",
      "  '0.125*\"network\" + 0.049*\"deep\" + 0.040*\"neural\" + 0.038*\"memory\" + '\n",
      "  '0.024*\"translation\" + 0.023*\"training\" + 0.022*\"convolutional\" + '\n",
      "  '0.021*\"weight\" + 0.020*\"layer\" + 0.015*\"unit\"'),\n",
      " (8,\n",
      "  '0.044*\"problem\" + 0.043*\"method\" + 0.030*\"function\" + 0.030*\"algorithm\" + '\n",
      "  '0.014*\"optimization\" + 0.013*\"stochastic\" + 0.011*\"show\" + 0.011*\"gradient\" '\n",
      "  '+ 0.011*\"convergence\" + 0.010*\"approximation\"'),\n",
      " (9,\n",
      "  '0.049*\"data\" + 0.027*\"distribution\" + 0.015*\"signal\" + 0.014*\"process\" + '\n",
      "  '0.013*\"bayesian\" + 0.013*\"analysis\" + 0.013*\"gaussian\" + 0.012*\"robust\" + '\n",
      "  '0.012*\"prior\" + 0.011*\"real\"')]\n",
      "[(0,\n",
      "  '0.127*\"feature\" + 0.096*\"space\" + 0.077*\"kernel\" + 0.040*\"selection\" + '\n",
      "  '0.027*\"manifold\" + 0.026*\"subset\" + 0.025*\"similarity\" + 0.020*\"robustness\" '\n",
      "  '+ 0.017*\"combination\" + 0.017*\"positive\"'),\n",
      " (1,\n",
      "  '0.063*\"task\" + 0.062*\"learning\" + 0.030*\"different\" + 0.025*\"word\" + '\n",
      "  '0.022*\"multiple\" + 0.022*\"representation\" + 0.018*\"learn\" + '\n",
      "  '0.018*\"sequence\" + 0.018*\"machine\" + 0.017*\"weight\"'),\n",
      " (2,\n",
      "  '0.032*\"decision\" + 0.025*\"human\" + 0.025*\"reward\" + 0.023*\"behavior\" + '\n",
      "  '0.023*\"making\" + 0.021*\"game\" + 0.017*\"context\" + 0.016*\"expert\" + '\n",
      "  '0.013*\"alternating\" + 0.013*\"expected\"'),\n",
      " (3,\n",
      "  '0.053*\"state\" + 0.048*\"system\" + 0.030*\"dynamic\" + 0.029*\"action\" + '\n",
      "  '0.024*\"time\" + 0.024*\"control\" + 0.020*\"agent\" + 0.017*\"observation\" + '\n",
      "  '0.014*\"working\" + 0.013*\"change\"'),\n",
      " (4,\n",
      "  '0.045*\"matrix\" + 0.034*\"data\" + 0.029*\"error\" + 0.028*\"loss\" + '\n",
      "  '0.028*\"estimation\" + 0.024*\"regression\" + 0.019*\"regularization\" + '\n",
      "  '0.018*\"signal\" + 0.016*\"metric\" + 0.016*\"analysis\"'),\n",
      " (5,\n",
      "  '0.076*\"algorithm\" + 0.026*\"bound\" + 0.023*\"problem\" + 0.022*\"number\" + '\n",
      "  '0.018*\"study\" + 0.017*\"result\" + 0.017*\"setting\" + 0.015*\"graph\" + '\n",
      "  '0.014*\"set\" + 0.014*\"time\"'),\n",
      " (6,\n",
      "  '0.080*\"topic\" + 0.070*\"binary\" + 0.070*\"tree\" + 0.038*\"hypothesis\" + '\n",
      "  '0.037*\"sampler\" + 0.031*\"annotator\" + 0.031*\"coefficient\" + '\n",
      "  '0.027*\"treewidth\" + 0.026*\"retrieval\" + 0.023*\"gibbs\"'),\n",
      " (7,\n",
      "  '0.172*\"network\" + 0.067*\"deep\" + 0.057*\"neural\" + 0.051*\"memory\" + '\n",
      "  '0.027*\"layer\" + 0.026*\"studied\" + 0.023*\"training\" + 0.021*\"unit\" + '\n",
      "  '0.019*\"recognition\" + 0.018*\"recurrent\"'),\n",
      " (8,\n",
      "  '0.072*\"exponential\" + 0.047*\"map\" + 0.042*\"bounded\" + 0.037*\"million\" + '\n",
      "  '0.036*\"lowrank\" + 0.029*\"user\" + 0.029*\"primal\" + 0.027*\"increasing\" + '\n",
      "  '0.027*\"coordinate\" + 0.025*\"combinatorial\"'),\n",
      " (9,\n",
      "  '0.039*\"rate\" + 0.035*\"information\" + 0.031*\"estimator\" + '\n",
      "  '0.027*\"statistical\" + 0.024*\"lower\" + 0.021*\"mean\" + 0.019*\"family\" + '\n",
      "  '0.019*\"measure\" + 0.017*\"risk\" + 0.017*\"theory\"'),\n",
      " (10,\n",
      "  '0.079*\"stochastic\" + 0.073*\"gradient\" + 0.066*\"convergence\" + 0.041*\"local\" '\n",
      "  '+ 0.038*\"descent\" + 0.032*\"scheme\" + 0.032*\"sampling\" + 0.027*\"global\" + '\n",
      "  '0.019*\"update\" + 0.019*\"optimization\"'),\n",
      " (11,\n",
      "  '0.125*\"model\" + 0.054*\"data\" + 0.028*\"structure\" + 0.024*\"inference\" + '\n",
      "  '0.020*\"process\" + 0.017*\"using\" + 0.017*\"variable\" + 0.016*\"bayesian\" + '\n",
      "  '0.011*\"latent\" + 0.011*\"parameter\"'),\n",
      " (12,\n",
      "  '0.042*\"neuron\" + 0.032*\"mechanism\" + 0.031*\"pattern\" + 0.030*\"brain\" + '\n",
      "  '0.028*\"neural\" + 0.026*\"spike\" + 0.023*\"document\" + 0.021*\"response\" + '\n",
      "  '0.020*\"functional\" + 0.020*\"connectivity\"'),\n",
      " (13,\n",
      "  '0.227*\"distribution\" + 0.064*\"density\" + 0.059*\"test\" + '\n",
      "  '0.047*\"nonparametric\" + 0.044*\"probability\" + 0.026*\"family\" + '\n",
      "  '0.023*\"marginal\" + 0.017*\"parametric\" + 0.017*\"statistic\" + 0.016*\"sample\"'),\n",
      " (14,\n",
      "  '0.089*\"image\" + 0.061*\"label\" + 0.042*\"training\" + 0.033*\"classification\" + '\n",
      "  '0.020*\"classifier\" + 0.019*\"generative\" + 0.018*\"convolutional\" + '\n",
      "  '0.018*\"code\" + 0.016*\"architecture\" + 0.016*\"dataset\"'),\n",
      " (15,\n",
      "  '0.070*\"norm\" + 0.045*\"measurement\" + 0.038*\"operator\" + 0.036*\"et\" + '\n",
      "  '0.036*\"al\" + 0.036*\"subspace\" + 0.035*\"tensor\" + 0.034*\"projection\" + '\n",
      "  '0.027*\"decomposition\" + 0.026*\"magnitude\"'),\n",
      " (16,\n",
      "  '0.037*\"power\" + 0.037*\"variance\" + 0.034*\"various\" + 0.032*\"automatically\" '\n",
      "  '+ 0.029*\"tool\" + 0.025*\"question\" + 0.020*\"bias\" + 0.019*\"web\" + '\n",
      "  '0.017*\"still\" + 0.016*\"threshold\"'),\n",
      " (17,\n",
      "  '0.046*\"method\" + 0.030*\"problem\" + 0.028*\"learning\" + 0.021*\"show\" + '\n",
      "  '0.020*\"function\" + 0.019*\"approach\" + 0.016*\"propose\" + 0.016*\"new\" + '\n",
      "  '0.014*\"result\" + 0.013*\"based\"'),\n",
      " (18,\n",
      "  '0.143*\"object\" + 0.049*\"detection\" + 0.043*\"ranking\" + 0.035*\"segmentation\" '\n",
      "  '+ 0.034*\"discriminative\" + 0.029*\"adversary\" + 0.029*\"category\" + '\n",
      "  '0.026*\"hash\" + 0.022*\"database\" + 0.022*\"supervised\"'),\n",
      " (19,\n",
      "  '0.072*\"group\" + 0.051*\"degree\" + 0.035*\"gamma\" + 0.027*\"side\" + '\n",
      "  '0.025*\"structural\" + 0.024*\"incomplete\" + 0.022*\"unified\" + '\n",
      "  '0.021*\"construction\" + 0.020*\"interesting\" + 0.019*\"shift\"')]\n",
      "[(45,\n",
      "  '0.178*\"control\" + 0.157*\"target\" + 0.106*\"working\" + 0.049*\"short\" + '\n",
      "  '0.032*\"tuning\" + 0.031*\"ideal\" + 0.021*\"analytically\" + 0.019*\"firing\" + '\n",
      "  '0.017*\"member\" + 0.016*\"distant\"'),\n",
      " (35,\n",
      "  '0.146*\"mechanism\" + 0.089*\"series\" + 0.056*\"iterative\" + 0.048*\"actually\" + '\n",
      "  '0.044*\"residual\" + 0.042*\"alignment\" + 0.035*\"independently\" + '\n",
      "  '0.034*\"directed\" + 0.033*\"reconstruct\" + 0.032*\"sensitive\"'),\n",
      " (46,\n",
      "  '0.092*\"video\" + 0.071*\"motion\" + 0.069*\"previously\" + 0.067*\"conditioned\" + '\n",
      "  '0.064*\"predict\" + 0.061*\"build\" + 0.050*\"bilingual\" + 0.038*\"corpus\" + '\n",
      "  '0.037*\"frame\" + 0.034*\"pixel\"'),\n",
      " (6,\n",
      "  '0.149*\"tree\" + 0.128*\"code\" + 0.047*\"divergence\" + 0.040*\"analyzing\" + '\n",
      "  '0.037*\"alpha\" + 0.035*\"statistically\" + 0.031*\"kl\" + 0.027*\"skewed\" + '\n",
      "  '0.027*\"deviation\" + 0.027*\"gene\"'),\n",
      " (28,\n",
      "  '0.166*\"weight\" + 0.111*\"layer\" + 0.064*\"injection\" + 0.040*\"cell\" + '\n",
      "  '0.037*\"encoding\" + 0.031*\"temporally\" + 0.026*\"normalization\" + '\n",
      "  '0.023*\"utilizes\" + 0.022*\"connection\" + 0.021*\"pointwise\"'),\n",
      " (23,\n",
      "  '0.098*\"hashing\" + 0.096*\"pair\" + 0.079*\"adversarial\" + 0.063*\"matching\" + '\n",
      "  '0.057*\"transition\" + 0.046*\"hash\" + 0.033*\"bit\" + 0.028*\"four\" + '\n",
      "  '0.027*\"storage\" + 0.026*\"generation\"'),\n",
      " (1,\n",
      "  '0.104*\"brain\" + 0.092*\"field\" + 0.068*\"connectivity\" + 0.065*\"region\" + '\n",
      "  '0.063*\"activity\" + 0.039*\"neuroscience\" + 0.029*\"powerlaw\" + '\n",
      "  '0.028*\"relatively\" + 0.027*\"voxels\" + 0.025*\"mouse\"'),\n",
      " (37,\n",
      "  '0.146*\"policy\" + 0.101*\"reward\" + 0.081*\"feedback\" + 0.068*\"preference\" + '\n",
      "  '0.055*\"reinforcement\" + 0.050*\"environment\" + 0.044*\"scenario\" + '\n",
      "  '0.038*\"value\" + 0.028*\"mdps\" + 0.026*\"mdp\"'),\n",
      " (39,\n",
      "  '0.242*\"signal\" + 0.064*\"population\" + 0.063*\"full\" + 0.049*\"transfer\" + '\n",
      "  '0.039*\"waveform\" + 0.039*\"processing\" + 0.038*\"introduces\" + 0.028*\"ground\" '\n",
      "  '+ 0.025*\"truth\" + 0.023*\"differentiable\"'),\n",
      " (48,\n",
      "  '0.119*\"pattern\" + 0.073*\"temporal\" + 0.055*\"advance\" + 0.047*\"area\" + '\n",
      "  '0.041*\"heldout\" + 0.040*\"event\" + 0.030*\"curve\" + 0.028*\"measured\" + '\n",
      "  '0.026*\"characteristic\" + 0.026*\"conditioning\"'),\n",
      " (41,\n",
      "  '0.112*\"number\" + 0.070*\"large\" + 0.049*\"memory\" + 0.045*\"small\" + '\n",
      "  '0.044*\"scale\" + 0.040*\"component\" + 0.039*\"multiple\" + 0.036*\"single\" + '\n",
      "  '0.026*\"reduction\" + 0.025*\"measurement\"'),\n",
      " (30,\n",
      "  '0.474*\"model\" + 0.090*\"inference\" + 0.060*\"bayesian\" + 0.037*\"variational\" '\n",
      "  '+ 0.037*\"probabilistic\" + 0.031*\"posterior\" + 0.023*\"generative\" + '\n",
      "  '0.016*\"dependency\" + 0.015*\"expectation\" + 0.015*\"match\"'),\n",
      " (10,\n",
      "  '0.093*\"analysis\" + 0.085*\"graph\" + 0.068*\"complexity\" + 0.060*\"sparse\" + '\n",
      "  '0.045*\"regularization\" + 0.043*\"lower\" + 0.039*\"constraint\" + '\n",
      "  '0.032*\"provides\" + 0.029*\"analyze\" + 0.024*\"highdimensional\"'),\n",
      " (12,\n",
      "  '0.071*\"information\" + 0.048*\"prediction\" + 0.038*\"accuracy\" + '\n",
      "  '0.032*\"computational\" + 0.030*\"within\" + 0.026*\"allows\" + '\n",
      "  '0.024*\"structured\" + 0.023*\"binary\" + 0.023*\"efficiently\" + 0.020*\"way\"'),\n",
      " (4,\n",
      "  '0.071*\"distribution\" + 0.050*\"sample\" + 0.047*\"parameter\" + '\n",
      "  '0.035*\"estimation\" + 0.032*\"random\" + 0.031*\"estimator\" + 0.029*\"prove\" + '\n",
      "  '0.028*\"probability\" + 0.028*\"statistical\" + 0.027*\"estimate\"'),\n",
      " (34,\n",
      "  '0.269*\"algorithm\" + 0.062*\"time\" + 0.033*\"guarantee\" + 0.032*\"theoretical\" '\n",
      "  '+ 0.027*\"sampling\" + 0.025*\"order\" + 0.018*\"give\" + 0.018*\"strategy\" + '\n",
      "  '0.018*\"best\" + 0.018*\"provide\"'),\n",
      " (27,\n",
      "  '0.137*\"data\" + 0.054*\"using\" + 0.048*\"structure\" + 0.036*\"framework\" + '\n",
      "  '0.034*\"process\" + 0.032*\"demonstrate\" + 0.028*\"learn\" + 0.027*\"use\" + '\n",
      "  '0.026*\"different\" + 0.026*\"present\"'),\n",
      " (21,\n",
      "  '0.053*\"study\" + 0.038*\"two\" + 0.027*\"show\" + 0.025*\"used\" + 0.015*\"set\" + '\n",
      "  '0.015*\"result\" + 0.013*\"given\" + 0.011*\"particular\" + 0.009*\"may\" + '\n",
      "  '0.009*\"thus\"'),\n",
      " (25,\n",
      "  '0.103*\"learning\" + 0.057*\"approach\" + 0.037*\"task\" + 0.035*\"performance\" + '\n",
      "  '0.035*\"propose\" + 0.029*\"method\" + 0.021*\"training\" + 0.020*\"machine\" + '\n",
      "  '0.019*\"application\" + 0.019*\"datasets\"'),\n",
      " (17,\n",
      "  '0.080*\"problem\" + 0.058*\"method\" + 0.042*\"function\" + 0.036*\"result\" + '\n",
      "  '0.034*\"show\" + 0.025*\"based\" + 0.024*\"also\" + 0.020*\"linear\" + 0.019*\"new\" '\n",
      "  '+ 0.018*\"optimal\"')]\n",
      "[(23,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (58,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (55,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (57,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (69,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (37,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (13,\n",
      "  '0.000*\"render\" + 0.000*\"messagepassing\" + 0.000*\"final\" + 0.000*\"chemical\" '\n",
      "  '+ 0.000*\"atom\" + 0.000*\"treebased\" + 0.000*\"timescales\" + '\n",
      "  '0.000*\"interacting\" + 0.000*\"reformulated\" + 0.000*\"rigid\"'),\n",
      " (7,\n",
      "  '0.263*\"improves\" + 0.189*\"attention\" + 0.183*\"jointly\" + 0.062*\"cnn\" + '\n",
      "  '0.055*\"convolution\" + 0.041*\"consequently\" + 0.038*\"focusing\" + 0.031*\"to\" '\n",
      "  '+ 0.029*\"look\" + 0.024*\"motor\"'),\n",
      " (63,\n",
      "  '0.282*\"reduction\" + 0.268*\"measurement\" + 0.111*\"dimensionality\" + '\n",
      "  '0.070*\"dynamical\" + 0.050*\"integrating\" + 0.048*\"embedded\" + '\n",
      "  '0.042*\"imaging\" + 0.034*\"overlap\" + 0.027*\"capability\" + '\n",
      "  '0.006*\"lowdimensional\"'),\n",
      " (30,\n",
      "  '0.196*\"alternative\" + 0.188*\"relevant\" + 0.187*\"expectation\" + '\n",
      "  '0.165*\"coefficient\" + 0.110*\"intractable\" + 0.093*\"modification\" + '\n",
      "  '0.000*\"treebased\" + 0.000*\"chemical\" + 0.000*\"final\" + 0.000*\"atom\"'),\n",
      " (51,\n",
      "  '0.274*\"setting\" + 0.119*\"high\" + 0.096*\"size\" + 0.088*\"strategy\" + '\n",
      "  '0.072*\"ie\" + 0.068*\"highdimensional\" + 0.050*\"requires\" + '\n",
      "  '0.042*\"dimensional\" + 0.034*\"increase\" + 0.033*\"asymptotically\"'),\n",
      " (40,\n",
      "  '0.137*\"sampling\" + 0.096*\"guarantee\" + 0.062*\"generalized\" + '\n",
      "  '0.062*\"numerical\" + 0.051*\"speed\" + 0.051*\"advantage\" + 0.050*\"exploit\" + '\n",
      "  '0.048*\"significant\" + 0.045*\"popular\" + 0.040*\"practice\"'),\n",
      " (90,\n",
      "  '0.328*\"number\" + 0.129*\"large\" + 0.115*\"order\" + 0.104*\"small\" + '\n",
      "  '0.069*\"faster\" + 0.033*\"magnitude\" + 0.032*\"superior\" + 0.028*\"energy\" + '\n",
      "  '0.025*\"product\" + 0.018*\"length\"'),\n",
      " (61,\n",
      "  '0.230*\"distribution\" + 0.156*\"sample\" + 0.111*\"estimation\" + '\n",
      "  '0.096*\"estimator\" + 0.083*\"estimate\" + 0.066*\"mean\" + 0.042*\"conditional\" + '\n",
      "  '0.039*\"consistency\" + 0.035*\"estimating\" + 0.031*\"covariance\"'),\n",
      " (50,\n",
      "  '0.156*\"optimization\" + 0.134*\"rate\" + 0.118*\"convergence\" + 0.098*\"convex\" '\n",
      "  '+ 0.054*\"risk\" + 0.054*\"achieves\" + 0.048*\"minimization\" + 0.039*\"parallel\" '\n",
      "  '+ 0.039*\"finite\" + 0.031*\"guarantee\"'),\n",
      " (76,\n",
      "  '0.541*\"data\" + 0.079*\"make\" + 0.075*\"robust\" + 0.073*\"real\" + '\n",
      "  '0.054*\"synthetic\" + 0.034*\"formulation\" + 0.018*\"assume\" + 0.015*\"nearly\" + '\n",
      "  '0.013*\"geometry\" + 0.013*\"outlier\"'),\n",
      " (71,\n",
      "  '0.495*\"model\" + 0.094*\"inference\" + 0.080*\"process\" + 0.062*\"bayesian\" + '\n",
      "  '0.039*\"variational\" + 0.038*\"probabilistic\" + 0.032*\"posterior\" + '\n",
      "  '0.026*\"nonparametric\" + 0.023*\"hierarchical\" + 0.017*\"describe\"'),\n",
      " (41,\n",
      "  '0.053*\"study\" + 0.044*\"result\" + 0.043*\"set\" + 0.036*\"analysis\" + '\n",
      "  '0.036*\"linear\" + 0.035*\"show\" + 0.032*\"provide\" + 0.031*\"also\" + '\n",
      "  '0.026*\"error\" + 0.025*\"case\"'),\n",
      " (78,\n",
      "  '0.168*\"algorithm\" + 0.124*\"problem\" + 0.123*\"learning\" + 0.036*\"framework\" '\n",
      "  '+ 0.031*\"new\" + 0.028*\"general\" + 0.022*\"machine\" + 0.022*\"application\" + '\n",
      "  '0.021*\"propose\" + 0.018*\"paper\"'),\n",
      " (74,\n",
      "  '0.078*\"method\" + 0.042*\"approach\" + 0.034*\"using\" + 0.028*\"show\" + '\n",
      "  '0.026*\"based\" + 0.026*\"present\" + 0.023*\"two\" + 0.021*\"novel\" + '\n",
      "  '0.021*\"demonstrate\" + 0.020*\"propose\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model5.print_topics())\n",
    "doc_lda = lda_model5[corpus]\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model10.print_topics())\n",
    "doc_lda = lda_model10[corpus]\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model20.print_topics())\n",
    "doc_lda = lda_model20[corpus]\n",
    "\n",
    "# Print the Keyword in the 50 topics\n",
    "pprint(lda_model50.print_topics())\n",
    "doc_lda = lda_model50[corpus]\n",
    "\n",
    "# Print the Keyword in the 100 topics\n",
    "pprint(lda_model100.print_topics())\n",
    "doc_lda = lda_model100[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.318990688575403\n",
      "\n",
      "Perplexity5:  -7.252011130616767\n",
      "\n",
      "Perplexity10:  -7.273317030403487\n",
      "\n",
      "Perplexity20:  -7.272599836321585\n",
      "\n",
      "Perplexity50:  -7.255893799215422\n",
      "\n",
      "Perplexity100:  -20.95130111001382\n",
      "\n",
      "Coherence Score:  0.2357683522987768\n",
      "\n",
      "Coherence Score5:  0.43434360700216124\n",
      "\n",
      "Coherence Score10:  0.3955419264337291\n",
      "\n",
      "Coherence Score20:  0.38508875505053064\n",
      "\n",
      "Coherence Score50:  0.38287359566897883\n",
      "\n",
      "Coherence Score100:  0.40607945589950356\n",
      "\n",
      "Coherence ScoreUMass:  -1.4635242209070833\n",
      "\n",
      "Coherence Score5UMass:  -1.8755717102328464\n",
      "\n",
      "Coherence Score10UMass:  -3.371076477597785\n",
      "\n",
      "Coherence Score20UMass:  -5.921739039959454\n",
      "\n",
      "Coherence Score50UMass:  -8.090264986114605\n",
      "\n",
      "Coherence Score100UMass:  -10.329427291841258\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity5: ', lda_model5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity10: ', lda_model10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity20: ', lda_model20.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity50: ', lda_model50.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity100: ', lda_model100.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100: ', coherence_lda)\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence ScoreUMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100UMass: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
