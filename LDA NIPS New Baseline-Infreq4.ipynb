{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inlin\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/Kornelius/Desktop/Data 2/nips-papers/papers.csv', header = 0, sep = ';', error_bad_lines=False)\n",
    "# Drop the columns not needed\n",
    "df = df.drop(columns=['id', 'event_type', 'pdf_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row by condition\n",
    "df = df[df.abstract != 'Abstract Missing']\n",
    "# Print out the first rows of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.abstract.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(data):\n",
    "    stop_free = \" \".join([i for i in data.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data = [clean(data).split() for data in data]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove characters and numbers\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16129 unique tokens: ['activity', 'also', 'analysis', 'applied', 'assume']...)\n"
     ]
    }
   ],
   "source": [
    "data = data_words\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 10 documents,\n",
    "id2word = corpora.Dictionary(data)\n",
    "id2word.filter_extremes(no_below = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data\n",
    "# Term Document Frequency and creating corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model20 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model50 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model100 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=100, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"algorithm\" + 0.014*\"model\" + 0.011*\"learning\" + 0.011*\"problem\" + '\n",
      "  '0.010*\"method\" + 0.009*\"data\" + 0.008*\"show\" + 0.006*\"result\" + '\n",
      "  '0.006*\"approach\" + 0.006*\"function\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 1 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.026*\"algorithm\" + 0.021*\"problem\" + 0.015*\"function\" + 0.014*\"bound\" + '\n",
      "  '0.013*\"result\" + 0.011*\"show\" + 0.010*\"matrix\" + 0.009*\"study\" + '\n",
      "  '0.008*\"setting\" + 0.008*\"linear\"'),\n",
      " (1,\n",
      "  '0.034*\"network\" + 0.029*\"image\" + 0.026*\"model\" + 0.016*\"neural\" + '\n",
      "  '0.013*\"deep\" + 0.012*\"object\" + 0.008*\"representation\" + 0.008*\"neuron\" + '\n",
      "  '0.007*\"signal\" + 0.007*\"using\"'),\n",
      " (2,\n",
      "  '0.044*\"method\" + 0.029*\"algorithm\" + 0.012*\"stochastic\" + 0.011*\"gradient\" '\n",
      "  '+ 0.010*\"approach\" + 0.010*\"optimization\" + 0.009*\"propose\" + '\n",
      "  '0.007*\"problem\" + 0.007*\"proposed\" + 0.007*\"time\"'),\n",
      " (3,\n",
      "  '0.045*\"learning\" + 0.024*\"data\" + 0.014*\"task\" + 0.013*\"approach\" + '\n",
      "  '0.011*\"problem\" + 0.011*\"label\" + 0.009*\"feature\" + 0.009*\"classification\" '\n",
      "  '+ 0.009*\"framework\" + 0.009*\"method\"'),\n",
      " (4,\n",
      "  '0.077*\"model\" + 0.023*\"distribution\" + 0.018*\"data\" + 0.017*\"inference\" + '\n",
      "  '0.016*\"process\" + 0.013*\"bayesian\" + 0.012*\"state\" + 0.010*\"variable\" + '\n",
      "  '0.010*\"structure\" + 0.008*\"latent\"')]\n",
      "[(0,\n",
      "  '0.036*\"algorithm\" + 0.027*\"problem\" + 0.019*\"method\" + 0.018*\"data\" + '\n",
      "  '0.012*\"result\" + 0.012*\"show\" + 0.011*\"number\" + 0.009*\"graph\" + '\n",
      "  '0.009*\"set\" + 0.008*\"propose\"'),\n",
      " (1,\n",
      "  '0.059*\"image\" + 0.030*\"model\" + 0.026*\"deep\" + 0.024*\"object\" + '\n",
      "  '0.024*\"network\" + 0.022*\"feature\" + 0.021*\"representation\" + 0.013*\"visual\" '\n",
      "  '+ 0.012*\"convolutional\" + 0.011*\"architecture\"'),\n",
      " (2,\n",
      "  '0.050*\"state\" + 0.046*\"system\" + 0.033*\"time\" + 0.028*\"dynamic\" + '\n",
      "  '0.028*\"action\" + 0.013*\"control\" + 0.013*\"change\" + 0.012*\"future\" + '\n",
      "  '0.012*\"series\" + 0.011*\"environment\"'),\n",
      " (3,\n",
      "  '0.060*\"learning\" + 0.023*\"approach\" + 0.020*\"task\" + 0.018*\"data\" + '\n",
      "  '0.017*\"method\" + 0.014*\"label\" + 0.013*\"performance\" + 0.012*\"show\" + '\n",
      "  '0.011*\"classification\" + 0.011*\"information\"'),\n",
      " (4,\n",
      "  '0.043*\"memory\" + 0.021*\"power\" + 0.020*\"score\" + 0.018*\"brain\" + '\n",
      "  '0.016*\"test\" + 0.016*\"functional\" + 0.016*\"difference\" + 0.016*\"million\" + '\n",
      "  '0.016*\"connectivity\" + 0.013*\"parallel\"'),\n",
      " (5,\n",
      "  '0.141*\"model\" + 0.036*\"inference\" + 0.030*\"distribution\" + 0.026*\"process\" '\n",
      "  '+ 0.026*\"variable\" + 0.024*\"bayesian\" + 0.017*\"latent\" + 0.015*\"structure\" '\n",
      "  '+ 0.015*\"variational\" + 0.015*\"probabilistic\"'),\n",
      " (6,\n",
      "  '0.036*\"function\" + 0.036*\"algorithm\" + 0.023*\"problem\" + 0.022*\"bound\" + '\n",
      "  '0.022*\"optimization\" + 0.018*\"gradient\" + 0.018*\"stochastic\" + '\n",
      "  '0.017*\"convergence\" + 0.016*\"loss\" + 0.015*\"method\"'),\n",
      " (7,\n",
      "  '0.049*\"online\" + 0.026*\"decision\" + 0.020*\"reward\" + 0.020*\"item\" + '\n",
      "  '0.018*\"agent\" + 0.017*\"game\" + 0.016*\"feedback\" + 0.014*\"strategy\" + '\n",
      "  '0.013*\"expert\" + 0.013*\"behavior\"'),\n",
      " (8,\n",
      "  '0.038*\"network\" + 0.022*\"model\" + 0.020*\"neural\" + 0.014*\"weight\" + '\n",
      "  '0.014*\"neuron\" + 0.013*\"input\" + 0.013*\"posterior\" + 0.012*\"signal\" + '\n",
      "  '0.009*\"connection\" + 0.009*\"pattern\"'),\n",
      " (9,\n",
      "  '0.025*\"sample\" + 0.021*\"matrix\" + 0.018*\"estimation\" + 0.017*\"distribution\" '\n",
      "  '+ 0.016*\"sparse\" + 0.015*\"estimator\" + 0.015*\"bound\" + 0.014*\"statistical\" '\n",
      "  '+ 0.013*\"error\" + 0.013*\"complexity\"')]\n",
      "[(0,\n",
      "  '0.034*\"function\" + 0.032*\"show\" + 0.030*\"linear\" + 0.019*\"optimal\" + '\n",
      "  '0.016*\"context\" + 0.016*\"space\" + 0.015*\"value\" + 0.015*\"result\" + '\n",
      "  '0.014*\"policy\" + 0.013*\"approximation\"'),\n",
      " (1,\n",
      "  '0.079*\"network\" + 0.070*\"image\" + 0.031*\"deep\" + 0.029*\"object\" + '\n",
      "  '0.028*\"representation\" + 0.028*\"word\" + 0.027*\"neural\" + 0.017*\"feature\" + '\n",
      "  '0.016*\"training\" + 0.015*\"convolutional\"'),\n",
      " (2,\n",
      "  '0.102*\"loss\" + 0.032*\"adaptive\" + 0.030*\"update\" + 0.020*\"respect\" + '\n",
      "  '0.020*\"extended\" + 0.019*\"typically\" + 0.019*\"fixed\" + 0.017*\"bias\" + '\n",
      "  '0.015*\"sense\" + 0.015*\"multiclass\"'),\n",
      " (3,\n",
      "  '0.081*\"learning\" + 0.051*\"method\" + 0.039*\"approach\" + 0.029*\"data\" + '\n",
      "  '0.027*\"task\" + 0.022*\"propose\" + 0.019*\"proposed\" + 0.019*\"label\" + '\n",
      "  '0.018*\"performance\" + 0.016*\"information\"'),\n",
      " (4,\n",
      "  '0.103*\"kernel\" + 0.068*\"density\" + 0.060*\"component\" + '\n",
      "  '0.051*\"nonparametric\" + 0.038*\"gp\" + 0.033*\"covariance\" + 0.027*\"gaussian\" '\n",
      "  '+ 0.024*\"marginal\" + 0.019*\"causal\" + 0.018*\"equivalence\"'),\n",
      " (5,\n",
      "  '0.130*\"inference\" + 0.093*\"variable\" + 0.040*\"mixture\" + 0.038*\"bayesian\" + '\n",
      "  '0.035*\"markov\" + 0.024*\"chain\" + 0.021*\"expectation\" + 0.019*\"approximate\" '\n",
      "  '+ 0.018*\"propagation\" + 0.017*\"monte\"'),\n",
      " (6,\n",
      "  '0.050*\"method\" + 0.042*\"function\" + 0.042*\"optimization\" + '\n",
      "  '0.037*\"stochastic\" + 0.035*\"rate\" + 0.034*\"gradient\" + 0.032*\"convergence\" '\n",
      "  '+ 0.026*\"convex\" + 0.022*\"objective\" + 0.018*\"descent\"'),\n",
      " (7,\n",
      "  '0.061*\"translation\" + 0.056*\"target\" + 0.038*\"working\" + '\n",
      "  '0.035*\"adversarial\" + 0.021*\"detect\" + 0.020*\"part\" + 0.019*\"worker\" + '\n",
      "  '0.017*\"recommendation\" + 0.017*\"incremental\" + 0.016*\"alignment\"'),\n",
      " (8,\n",
      "  '0.125*\"model\" + 0.035*\"distribution\" + 0.028*\"data\" + 0.018*\"parameter\" + '\n",
      "  '0.017*\"structure\" + 0.017*\"using\" + 0.015*\"process\" + 0.013*\"use\" + '\n",
      "  '0.013*\"sampling\" + 0.011*\"family\"'),\n",
      " (9,\n",
      "  '0.111*\"graph\" + 0.052*\"structure\" + 0.039*\"structured\" + 0.037*\"output\" + '\n",
      "  '0.034*\"related\" + 0.029*\"group\" + 0.023*\"map\" + 0.019*\"in\" + '\n",
      "  '0.018*\"relationship\" + 0.016*\"relation\"'),\n",
      " (10,\n",
      "  '0.045*\"human\" + 0.034*\"behavior\" + 0.030*\"mechanism\" + 0.029*\"game\" + '\n",
      "  '0.024*\"observation\" + 0.022*\"expert\" + 0.020*\"account\" + 0.020*\"functional\" '\n",
      "  '+ 0.019*\"effect\" + 0.016*\"interaction\"'),\n",
      " (11,\n",
      "  '0.076*\"algorithm\" + 0.052*\"problem\" + 0.022*\"study\" + 0.021*\"result\" + '\n",
      "  '0.019*\"number\" + 0.016*\"class\" + 0.015*\"framework\" + 0.015*\"show\" + '\n",
      "  '0.013*\"set\" + 0.013*\"new\"'),\n",
      " (12,\n",
      "  '0.047*\"likelihood\" + 0.046*\"node\" + 0.043*\"maximum\" + 0.042*\"active\" + '\n",
      "  '0.037*\"query\" + 0.035*\"oracle\" + 0.034*\"random\" + 0.032*\"field\" + '\n",
      "  '0.028*\"element\" + 0.027*\"criterion\"'),\n",
      " (13,\n",
      "  '0.052*\"measure\" + 0.051*\"metric\" + 0.051*\"test\" + 0.048*\"embeddings\" + '\n",
      "  '0.036*\"classifier\" + 0.034*\"embedding\" + 0.028*\"power\" + 0.027*\"manifold\" + '\n",
      "  '0.027*\"distance\" + 0.025*\"space\"'),\n",
      " (14,\n",
      "  '0.046*\"memory\" + 0.037*\"weight\" + 0.032*\"binary\" + 0.023*\"studied\" + '\n",
      "  '0.023*\"various\" + 0.023*\"hidden\" + 0.019*\"unit\" + 0.018*\"tensor\" + '\n",
      "  '0.018*\"language\" + 0.017*\"benefit\"'),\n",
      " (15,\n",
      "  '0.116*\"bound\" + 0.052*\"regret\" + 0.034*\"lower\" + 0.033*\"setting\" + '\n",
      "  '0.029*\"bandit\" + 0.019*\"best\" + 0.019*\"optimal\" + 0.016*\"arm\" + '\n",
      "  '0.016*\"known\" + 0.015*\"guarantee\"'),\n",
      " (16,\n",
      "  '0.050*\"item\" + 0.047*\"search\" + 0.039*\"hierarchical\" + 0.035*\"hashing\" + '\n",
      "  '0.032*\"communication\" + 0.028*\"video\" + 0.022*\"motion\" + 0.021*\"previously\" '\n",
      "  '+ 0.020*\"conditioned\" + 0.020*\"thousand\"'),\n",
      " (17,\n",
      "  '0.056*\"neuron\" + 0.035*\"neural\" + 0.034*\"spike\" + 0.033*\"pattern\" + '\n",
      "  '0.026*\"connectivity\" + 0.024*\"activity\" + 0.023*\"population\" + '\n",
      "  '0.022*\"stimulus\" + 0.021*\"brain\" + 0.018*\"correlation\"'),\n",
      " (18,\n",
      "  '0.040*\"matrix\" + 0.033*\"sample\" + 0.025*\"estimation\" + 0.023*\"sparse\" + '\n",
      "  '0.022*\"estimator\" + 0.019*\"error\" + 0.017*\"regularization\" + '\n",
      "  '0.017*\"statistical\" + 0.017*\"vector\" + 0.016*\"noise\"'),\n",
      " (19,\n",
      "  '0.052*\"state\" + 0.047*\"system\" + 0.030*\"time\" + 0.030*\"dynamic\" + '\n",
      "  '0.029*\"decision\" + 0.029*\"action\" + 0.024*\"control\" + 0.023*\"tree\" + '\n",
      "  '0.020*\"agent\" + 0.018*\"feedback\"')]\n",
      "[(35,\n",
      "  '0.279*\"signal\" + 0.088*\"functional\" + 0.057*\"combining\" + 0.051*\"synaptic\" '\n",
      "  '+ 0.037*\"play\" + 0.032*\"rich\" + 0.029*\"role\" + 0.023*\"strength\" + '\n",
      "  '0.022*\"episode\" + 0.019*\"recall\"'),\n",
      " (23,\n",
      "  '0.109*\"spike\" + 0.079*\"recurrent\" + 0.057*\"path\" + 0.049*\"spiking\" + '\n",
      "  '0.049*\"cell\" + 0.044*\"waveform\" + 0.044*\"importantly\" + 0.040*\"encoding\" + '\n",
      "  '0.036*\"varying\" + 0.033*\"curve\"'),\n",
      " (41,\n",
      "  '0.089*\"hashing\" + 0.081*\"communication\" + 0.080*\"learner\" + 0.078*\"working\" '\n",
      "  '+ 0.064*\"belief\" + 0.030*\"powerlaw\" + 0.026*\"extensively\" + 0.024*\"affect\" '\n",
      "  '+ 0.021*\"fairness\" + 0.018*\"fair\"'),\n",
      " (38,\n",
      "  '0.152*\"policy\" + 0.109*\"control\" + 0.105*\"reward\" + 0.084*\"feedback\" + '\n",
      "  '0.071*\"preference\" + 0.057*\"reinforcement\" + 0.041*\"value\" + '\n",
      "  '0.032*\"combined\" + 0.028*\"experimentally\" + 0.021*\"resource\"'),\n",
      " (15,\n",
      "  '0.166*\"word\" + 0.093*\"translation\" + 0.055*\"language\" + 0.049*\"completion\" '\n",
      "  '+ 0.043*\"reconstruction\" + 0.037*\"variation\" + 0.029*\"bilingual\" + '\n",
      "  '0.028*\"introduces\" + 0.027*\"sentence\" + 0.026*\"shape\"'),\n",
      " (39,\n",
      "  '0.266*\"kernel\" + 0.038*\"boosting\" + 0.038*\"shared\" + 0.038*\"successfully\" + '\n",
      "  '0.034*\"drawn\" + 0.033*\"space\" + 0.032*\"solved\" + 0.027*\"byproduct\" + '\n",
      "  '0.026*\"multivariate\" + 0.025*\"hilbert\"'),\n",
      " (5,\n",
      "  '0.156*\"selection\" + 0.080*\"alternative\" + 0.076*\"additional\" + '\n",
      "  '0.065*\"scenario\" + 0.053*\"dependence\" + 0.050*\"multitask\" + 0.049*\"simpler\" '\n",
      "  '+ 0.047*\"expensive\" + 0.044*\"share\" + 0.043*\"batch\"'),\n",
      " (2,\n",
      "  '0.136*\"output\" + 0.061*\"annotator\" + 0.057*\"relation\" + 0.036*\"database\" + '\n",
      "  '0.033*\"streaming\" + 0.032*\"open\" + 0.029*\"lp\" + 0.027*\"development\" + '\n",
      "  '0.026*\"switching\" + 0.023*\"discovery\"'),\n",
      " (20,\n",
      "  '0.165*\"clustering\" + 0.105*\"cluster\" + 0.067*\"trajectory\" + 0.063*\"series\" '\n",
      "  '+ 0.060*\"phase\" + 0.049*\"total\" + 0.046*\"advance\" + 0.038*\"implement\" + '\n",
      "  '0.030*\"allowed\" + 0.025*\"consensus\"'),\n",
      " (31,\n",
      "  '0.103*\"code\" + 0.063*\"sampler\" + 0.051*\"relative\" + 0.046*\"gamma\" + '\n",
      "  '0.039*\"gibbs\" + 0.037*\"hash\" + 0.033*\"bayes\" + 0.028*\"count\" + '\n",
      "  '0.027*\"construction\" + 0.027*\"filtering\"'),\n",
      " (28,\n",
      "  '0.095*\"family\" + 0.094*\"point\" + 0.060*\"exponential\" + 0.060*\"scheme\" + '\n",
      "  '0.050*\"computationally\" + 0.041*\"arbitrary\" + 0.032*\"privacy\" + '\n",
      "  '0.031*\"million\" + 0.031*\"em\" + 0.030*\"lowrank\"'),\n",
      " (12,\n",
      "  '0.112*\"random\" + 0.086*\"sampling\" + 0.067*\"sequence\" + 0.050*\"likelihood\" + '\n",
      "  '0.048*\"target\" + 0.048*\"evaluation\" + 0.047*\"conditional\" + 0.046*\"maximum\" '\n",
      "  '+ 0.044*\"active\" + 0.035*\"us\"'),\n",
      " (49,\n",
      "  '0.193*\"task\" + 0.106*\"error\" + 0.095*\"training\" + 0.066*\"generalization\" + '\n",
      "  '0.045*\"significantly\" + 0.036*\"reduction\" + 0.033*\"adaptive\" + '\n",
      "  '0.033*\"improvement\" + 0.028*\"speed\" + 0.023*\"faster\"'),\n",
      " (34,\n",
      "  '0.104*\"setting\" + 0.075*\"parameter\" + 0.068*\"estimation\" + '\n",
      "  '0.059*\"estimator\" + 0.052*\"statistical\" + 0.051*\"estimate\" + 0.045*\"lower\" '\n",
      "  '+ 0.041*\"mean\" + 0.039*\"procedure\" + 0.032*\"risk\"'),\n",
      " (19,\n",
      "  '0.091*\"bound\" + 0.049*\"stochastic\" + 0.043*\"general\" + 0.041*\"regret\" + '\n",
      "  '0.039*\"provide\" + 0.037*\"case\" + 0.036*\"guarantee\" + 0.025*\"known\" + '\n",
      "  '0.023*\"bandit\" + 0.022*\"algorithm\"'),\n",
      " (14,\n",
      "  '0.053*\"feature\" + 0.035*\"large\" + 0.035*\"space\" + 0.031*\"datasets\" + '\n",
      "  '0.027*\"representation\" + 0.023*\"high\" + 0.023*\"performance\" + '\n",
      "  '0.023*\"dimension\" + 0.020*\"scale\" + 0.019*\"stateoftheart\"'),\n",
      " (43,\n",
      "  '0.160*\"learning\" + 0.076*\"function\" + 0.058*\"method\" + 0.043*\"class\" + '\n",
      "  '0.040*\"approach\" + 0.030*\"performance\" + 0.029*\"classification\" + '\n",
      "  '0.029*\"loss\" + 0.028*\"machine\" + 0.022*\"cost\"'),\n",
      " (7,\n",
      "  '0.058*\"study\" + 0.036*\"two\" + 0.036*\"result\" + 0.029*\"show\" + 0.019*\"given\" '\n",
      "  '+ 0.013*\"without\" + 0.010*\"goal\" + 0.010*\"work\" + 0.009*\"solving\" + '\n",
      "  '0.009*\"fixed\"'),\n",
      " (36,\n",
      "  '0.095*\"model\" + 0.063*\"data\" + 0.028*\"using\" + 0.027*\"method\" + '\n",
      "  '0.024*\"framework\" + 0.021*\"structure\" + 0.019*\"approach\" + 0.017*\"use\" + '\n",
      "  '0.016*\"experiment\" + 0.015*\"propose\"'),\n",
      " (0,\n",
      "  '0.094*\"algorithm\" + 0.073*\"problem\" + 0.029*\"show\" + 0.023*\"set\" + '\n",
      "  '0.023*\"number\" + 0.021*\"time\" + 0.020*\"also\" + 0.019*\"linear\" + 0.019*\"new\" '\n",
      "  '+ 0.016*\"analysis\"')]\n",
      "[(83,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (47,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (74,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (24,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (40,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (73,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (17,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (11,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (91,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (67,\n",
      "  '0.000*\"exhibiting\" + 0.000*\"cycle\" + 0.000*\"decade\" + 0.000*\"date\" + '\n",
      "  '0.000*\"timescale\" + 0.000*\"switching\" + 0.000*\"switch\" + 0.000*\"member\" + '\n",
      "  '0.000*\"motif\" + 0.000*\"episode\"'),\n",
      " (65,\n",
      "  '0.328*\"network\" + 0.131*\"neural\" + 0.128*\"deep\" + 0.099*\"memory\" + '\n",
      "  '0.052*\"layer\" + 0.048*\"various\" + 0.048*\"hidden\" + 0.044*\"recognition\" + '\n",
      "  '0.020*\"regime\" + 0.020*\"includes\"'),\n",
      " (56,\n",
      "  '0.216*\"optimal\" + 0.101*\"action\" + 0.076*\"goal\" + 0.070*\"agent\" + '\n",
      "  '0.055*\"oracle\" + 0.042*\"must\" + 0.039*\"environment\" + 0.035*\"exploration\" + '\n",
      "  '0.034*\"fundamental\" + 0.028*\"implementation\"'),\n",
      " (2,\n",
      "  '0.190*\"optimization\" + 0.163*\"rate\" + 0.143*\"convergence\" + 0.119*\"convex\" '\n",
      "  '+ 0.067*\"achieves\" + 0.066*\"risk\" + 0.059*\"minimization\" + 0.027*\"sum\" + '\n",
      "  '0.027*\"smooth\" + 0.019*\"accelerated\"'),\n",
      " (82,\n",
      "  '0.590*\"learning\" + 0.103*\"machine\" + 0.101*\"learn\" + 0.080*\"word\" + '\n",
      "  '0.040*\"learned\" + 0.031*\"evaluate\" + 0.017*\"build\" + 0.016*\"research\" + '\n",
      "  '0.007*\"supervision\" + 0.004*\"teacher\"'),\n",
      " (70,\n",
      "  '0.619*\"model\" + 0.048*\"probabilistic\" + 0.044*\"via\" + 0.032*\"capture\" + '\n",
      "  '0.028*\"generalized\" + 0.022*\"extension\" + 0.022*\"recover\" + 0.020*\"popular\" '\n",
      "  '+ 0.018*\"relationship\" + 0.017*\"understanding\"'),\n",
      " (32,\n",
      "  '0.551*\"algorithm\" + 0.114*\"setting\" + 0.079*\"online\" + 0.037*\"exponential\" '\n",
      "  '+ 0.031*\"variant\" + 0.024*\"improvement\" + 0.023*\"improved\" + '\n",
      "  '0.021*\"requires\" + 0.011*\"logarithmic\" + 0.011*\"additive\"'),\n",
      " (51,\n",
      "  '0.105*\"linear\" + 0.083*\"well\" + 0.069*\"approximation\" + 0.042*\"first\" + '\n",
      "  '0.038*\"derive\" + 0.038*\"assumption\" + 0.036*\"scheme\" + '\n",
      "  '0.030*\"computationally\" + 0.027*\"independent\" + 0.026*\"highdimensional\"'),\n",
      " (28,\n",
      "  '0.092*\"time\" + 0.087*\"sample\" + 0.075*\"general\" + 0.065*\"case\" + '\n",
      "  '0.054*\"guarantee\" + 0.054*\"theoretical\" + 0.038*\"known\" + 0.037*\"error\" + '\n",
      "  '0.035*\"order\" + 0.031*\"norm\"'),\n",
      " (62,\n",
      "  '0.066*\"problem\" + 0.041*\"approach\" + 0.029*\"show\" + 0.027*\"number\" + '\n",
      "  '0.024*\"present\" + 0.023*\"class\" + 0.020*\"using\" + 0.020*\"two\" + 0.019*\"set\" '\n",
      "  '+ 0.018*\"provide\"'),\n",
      " (85,\n",
      "  '0.068*\"method\" + 0.060*\"data\" + 0.031*\"result\" + 0.029*\"propose\" + '\n",
      "  '0.027*\"show\" + 0.024*\"study\" + 0.023*\"based\" + 0.021*\"also\" + '\n",
      "  '0.021*\"performance\" + 0.020*\"new\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model5.print_topics())\n",
    "doc_lda = lda_model5[corpus]\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model10.print_topics())\n",
    "doc_lda = lda_model10[corpus]\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model20.print_topics())\n",
    "doc_lda = lda_model20[corpus]\n",
    "\n",
    "# Print the Keyword in the 50 topics\n",
    "pprint(lda_model50.print_topics())\n",
    "doc_lda = lda_model50[corpus]\n",
    "\n",
    "# Print the Keyword in the 100 topics\n",
    "pprint(lda_model100.print_topics())\n",
    "doc_lda = lda_model100[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.3703725717709005\n",
      "\n",
      "Perplexity5:  -7.31045927188444\n",
      "\n",
      "Perplexity10:  -7.32027004113019\n",
      "\n",
      "Perplexity20:  -7.3266768473562776\n",
      "\n",
      "Perplexity50:  -7.314854691053444\n",
      "\n",
      "Perplexity100:  -21.693198051909416\n",
      "\n",
      "Coherence Score:  0.2357683522987768\n",
      "\n",
      "Coherence Score5:  0.4183360892709917\n",
      "\n",
      "Coherence Score10:  0.4281102848838854\n",
      "\n",
      "Coherence Score20:  0.3963062272877122\n",
      "\n",
      "Coherence Score50:  0.38316147840789305\n",
      "\n",
      "Coherence Score100:  0.4368142603802452\n",
      "\n",
      "Coherence ScoreUMass:  -1.4635242209070833\n",
      "\n",
      "Coherence Score5UMass:  -1.8416943481022419\n",
      "\n",
      "Coherence Score10UMass:  -2.977342995177242\n",
      "\n",
      "Coherence Score20UMass:  -5.502390332663113\n",
      "\n",
      "Coherence Score50UMass:  -8.12733223867958\n",
      "\n",
      "Coherence Score100UMass:  -10.897611551825712\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity5: ', lda_model5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity10: ', lda_model10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity20: ', lda_model20.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity50: ', lda_model50.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity100: ', lda_model100.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100: ', coherence_lda)\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence ScoreUMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100UMass: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
