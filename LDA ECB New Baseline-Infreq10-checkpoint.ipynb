{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kornelius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inlin\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Press</th>\n",
       "      <th>qa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>Ladies and gentlemen, in line with our stated ...</td>\n",
       "      <td>We have no indications based on the most recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>Ladies and gentlemen, as in previous months, t...</td>\n",
       "      <td>The answer to the first question is that it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>Ladies and gentlemen, the Vice-President and I...</td>\n",
       "      <td>We simply felt that the announcement of a rang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>Ladies and gentlemen, the Vice-President and I...</td>\n",
       "      <td>No, that is not a signal that it is supposed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>Ladies and gentlemen, the Vice-President and I...</td>\n",
       "      <td>The main problems have not been technical ones...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date                                              Press  \\\n",
       "0  1998  Ladies and gentlemen, in line with our stated ...   \n",
       "1  1998  Ladies and gentlemen, as in previous months, t...   \n",
       "2  1998  Ladies and gentlemen, the Vice-President and I...   \n",
       "3  1998  Ladies and gentlemen, the Vice-President and I...   \n",
       "4  1999  Ladies and gentlemen, the Vice-President and I...   \n",
       "\n",
       "                                                  qa  \n",
       "0  We have no indications based on the most recen...  \n",
       "1  The answer to the first question is that it is...  \n",
       "2  We simply felt that the announcement of a rang...  \n",
       "3  No, that is not a signal that it is supposed t...  \n",
       "4  The main problems have not been technical ones...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_excel('C:/Users/Kornelius/Desktop/Data 2/ecbpr/Warin_Sanger_ECB.xlsx') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.values.tolist()\n",
    "# Drop a row by condition\n",
    "# Print out the first rows of papers\n",
    "#df = df[df.press]\n",
    "df = df['Press']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(df):\n",
    "    stop_free = \" \".join([i for i in df.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data = [clean(df).split()for df in df]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove characters and numbers\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4394 unique tokens: ['able', 'acceptable', 'accompanied', 'according', 'account']...)\n"
     ]
    }
   ],
   "source": [
    "data = data_words\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data)\n",
    "print((id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data\n",
    "# less than 10 documents,\n",
    "id2word = corpora.Dictionary(texts)\n",
    "id2word.filter_extremes(no_below = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data\n",
    "# Term Document Frequency and creating corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kornelius\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda_model5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model20 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model50 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model100 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=100, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.010*\"projection\" + 0.009*\"staff\" + 0.009*\"purchase\" + 0.008*\"june\" + '\n",
      "  '0.007*\"march\" + 0.006*\"asset\" + 0.006*\"sustained\" + 0.006*\"accommodative\" + '\n",
      "  '0.006*\"contribute\" + 0.005*\"september\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 1 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"projection\" + 0.015*\"staff\" + 0.014*\"range\" + 0.010*\"accommodative\" '\n",
      "  '+ 0.010*\"tension\" + 0.009*\"anchoring\" + 0.008*\"job\" + 0.008*\"contained\" + '\n",
      "  '0.008*\"creation\" + 0.008*\"commodity\"'),\n",
      " (1,\n",
      "  '0.012*\"subdued\" + 0.010*\"commodity\" + 0.010*\"weak\" + 0.009*\"negative\" + '\n",
      "  '0.008*\"below\" + 0.008*\"to\" + 0.008*\"aim\" + 0.006*\"september\" + '\n",
      "  '0.006*\"gradual\" + 0.006*\"policyrelevant\"'),\n",
      " (2,\n",
      "  '0.007*\"consumer\" + 0.006*\"trend\" + 0.004*\"world\" + 0.004*\"average\" + '\n",
      "  '0.004*\"background\" + 0.004*\"course\" + 0.004*\"member\" + 0.004*\"thus\" + '\n",
      "  '0.004*\"state\" + 0.004*\"exchange\"'),\n",
      " (3,\n",
      "  '0.018*\"purchase\" + 0.016*\"staff\" + 0.016*\"projection\" + 0.015*\"june\" + '\n",
      "  '0.013*\"march\" + 0.012*\"sustained\" + 0.012*\"path\" + 0.011*\"asset\" + '\n",
      "  '0.011*\"across\" + 0.011*\"accommodative\"'),\n",
      " (4,\n",
      "  '0.032*\"october\" + 0.029*\"purchase\" + 0.029*\"september\" + 0.028*\"december\" + '\n",
      "  '0.022*\"operation\" + 0.021*\"security\" + 0.018*\"maturity\" + 0.016*\"january\" + '\n",
      "  '0.015*\"facility\" + 0.015*\"programme\"')]\n",
      "[(0,\n",
      "  '0.001*\"governance\" + 0.001*\"projection\" + 0.001*\"staff\" + 0.001*\"consumer\" '\n",
      "  '+ 0.001*\"confirmed\" + 0.001*\"trend\" + 0.001*\"march\" + 0.001*\"longer\" + '\n",
      "  '0.001*\"course\" + 0.001*\"forecast\"'),\n",
      " (1,\n",
      "  '0.006*\"sound\" + 0.006*\"commodity\" + 0.006*\"volatility\" + 0.005*\"firm\" + '\n",
      "  '0.005*\"accordingly\" + 0.005*\"longer\" + 0.005*\"competition\" + 0.005*\"robust\" '\n",
      "  '+ 0.005*\"fully\" + 0.005*\"secondround\"'),\n",
      " (2,\n",
      "  '0.012*\"pillar\" + 0.010*\"consumer\" + 0.008*\"exchange\" + 0.008*\"course\" + '\n",
      "  '0.008*\"trend\" + 0.007*\"member\" + 0.007*\"forecast\" + 0.007*\"framework\" + '\n",
      "  '0.006*\"moderation\" + 0.006*\"average\"'),\n",
      " (3,\n",
      "  '0.026*\"march\" + 0.020*\"april\" + 0.016*\"june\" + 0.014*\"across\" + '\n",
      "  '0.012*\"contribute\" + 0.012*\"staff\" + 0.012*\"accommodative\" + '\n",
      "  '0.011*\"purchase\" + 0.010*\"path\" + 0.010*\"projection\"'),\n",
      " (4,\n",
      "  '0.026*\"purchase\" + 0.017*\"projection\" + 0.016*\"staff\" + 0.015*\"asset\" + '\n",
      "  '0.014*\"sustained\" + 0.013*\"june\" + 0.012*\"september\" + 0.012*\"path\" + '\n",
      "  '0.011*\"july\" + 0.010*\"programme\"'),\n",
      " (5,\n",
      "  '0.013*\"subdued\" + 0.012*\"to\" + 0.011*\"below\" + 0.011*\"aim\" + '\n",
      "  '0.010*\"projection\" + 0.009*\"weak\" + 0.009*\"commodity\" + 0.008*\"negative\" + '\n",
      "  '0.008*\"staff\" + 0.007*\"june\"'),\n",
      " (6,\n",
      "  '0.017*\"january\" + 0.012*\"operation\" + 0.012*\"central\" + 0.011*\"facility\" + '\n",
      "  '0.011*\"december\" + 0.010*\"national\" + 0.009*\"value\" + 0.008*\"reference\" + '\n",
      "  '0.008*\"bond\" + 0.008*\"system\"'),\n",
      " (7,\n",
      "  '0.001*\"operation\" + 0.001*\"january\" + 0.001*\"trend\" + 0.001*\"march\" + '\n",
      "  '0.001*\"refinancing\" + 0.001*\"job\" + 0.001*\"consumer\" + 0.001*\"position\" + '\n",
      "  '0.001*\"allotment\" + 0.001*\"situation\"'),\n",
      " (8,\n",
      "  '0.051*\"projection\" + 0.032*\"staff\" + 0.020*\"ample\" + 0.019*\"range\" + '\n",
      "  '0.018*\"projected\" + 0.014*\"longer\" + 0.012*\"manner\" + 0.012*\"revised\" + '\n",
      "  '0.012*\"assumption\" + 0.012*\"timely\"'),\n",
      " (9,\n",
      "  '0.089*\"operation\" + 0.087*\"fixed\" + 0.063*\"tender\" + 0.056*\"refinancing\" + '\n",
      "  '0.056*\"procedure\" + 0.046*\"maintenance\" + 0.039*\"mros\" + 0.033*\"threemonth\" '\n",
      "  '+ 0.032*\"specialterm\" + 0.028*\"end\"')]\n",
      "[(0,\n",
      "  '0.001*\"consumer\" + 0.001*\"longer\" + 0.001*\"staff\" + 0.001*\"confirmed\" + '\n",
      "  '0.001*\"projection\" + 0.001*\"ample\" + 0.001*\"vigilance\" + 0.001*\"trend\" + '\n",
      "  '0.001*\"forecast\" + 0.001*\"background\"'),\n",
      " (1,\n",
      "  '0.016*\"path\" + 0.016*\"across\" + 0.013*\"accommodative\" + 0.013*\"including\" + '\n",
      "  '0.012*\"sustained\" + 0.012*\"purchase\" + 0.012*\"august\" + 0.011*\"asset\" + '\n",
      "  '0.011*\"boost\" + 0.011*\"improve\"'),\n",
      " (2,\n",
      "  '0.018*\"gradual\" + 0.018*\"world\" + 0.015*\"finance\" + 0.014*\"effort\" + '\n",
      "  '0.013*\"forecast\" + 0.013*\"framework\" + 0.012*\"outside\" + 0.012*\"maintained\" '\n",
      "  '+ 0.011*\"course\" + 0.010*\"treaty\"'),\n",
      " (3,\n",
      "  '0.044*\"projection\" + 0.044*\"staff\" + 0.033*\"june\" + 0.031*\"march\" + '\n",
      "  '0.017*\"april\" + 0.014*\"contribute\" + 0.013*\"foresee\" + 0.013*\"july\" + '\n",
      "  '0.012*\"slightly\" + 0.012*\"positively\"'),\n",
      " (4,\n",
      "  '0.017*\"january\" + 0.014*\"exchange\" + 0.014*\"operation\" + 0.013*\"facility\" + '\n",
      "  '0.013*\"consumer\" + 0.012*\"december\" + 0.010*\"bond\" + 0.010*\"november\" + '\n",
      "  '0.010*\"value\" + 0.009*\"release\"'),\n",
      " (5,\n",
      "  '0.031*\"purchase\" + 0.017*\"september\" + 0.017*\"asset\" + 0.017*\"below\" + '\n",
      "  '0.016*\"to\" + 0.013*\"programme\" + 0.013*\"projection\" + 0.013*\"sustained\" + '\n",
      "  '0.012*\"staff\" + 0.012*\"net\"'),\n",
      " (6,\n",
      "  '0.001*\"january\" + 0.001*\"december\" + 0.001*\"operation\" + 0.001*\"shifted\" + '\n",
      "  '0.001*\"banknote\" + 0.001*\"february\" + 0.001*\"settled\" + 0.001*\"target\" + '\n",
      "  '0.001*\"day\" + 0.001*\"trend\"'),\n",
      " (7,\n",
      "  '0.001*\"january\" + 0.001*\"consumer\" + 0.001*\"trend\" + 0.001*\"operation\" + '\n",
      "  '0.001*\"march\" + 0.001*\"background\" + 0.001*\"three\" + 0.001*\"eu\" + '\n",
      "  '0.001*\"side\" + 0.001*\"system\"'),\n",
      " (8,\n",
      "  '0.001*\"projection\" + 0.001*\"ample\" + 0.001*\"staff\" + 0.001*\"contribution\" + '\n",
      "  '0.001*\"average\" + 0.001*\"longer\" + 0.001*\"timely\" + 0.001*\"thus\" + '\n",
      "  '0.001*\"creation\" + 0.001*\"consumer\"'),\n",
      " (9,\n",
      "  '0.001*\"pillar\" + 0.001*\"average\" + 0.001*\"difficult\" + 0.001*\"course\" + '\n",
      "  '0.001*\"exchange\" + 0.001*\"world\" + 0.001*\"consumer\" + 0.001*\"globally\" + '\n",
      "  '0.001*\"banknote\" + 0.001*\"threemonth\"'),\n",
      " (10,\n",
      "  '0.031*\"projection\" + 0.026*\"vigilance\" + 0.017*\"side\" + 0.016*\"secondround\" '\n",
      "  '+ 0.016*\"staff\" + 0.015*\"short\" + 0.014*\"contained\" + 0.014*\"pricesetting\" '\n",
      "  '+ 0.013*\"consumer\" + 0.012*\"governance\"'),\n",
      " (11,\n",
      "  '0.001*\"central\" + 0.001*\"regulation\" + 0.001*\"legal\" + 0.001*\"framework\" + '\n",
      "  '0.001*\"three\" + 0.001*\"member\" + 0.001*\"difficult\" + 0.001*\"value\" + '\n",
      "  '0.001*\"position\" + 0.001*\"pillar\"'),\n",
      " (12,\n",
      "  '0.026*\"february\" + 0.024*\"weak\" + 0.021*\"march\" + 0.019*\"january\" + '\n",
      "  '0.017*\"weaker\" + 0.014*\"signal\" + 0.012*\"made\" + 0.011*\"sale\" + '\n",
      "  '0.011*\"indicates\" + 0.011*\"december\"'),\n",
      " (13,\n",
      "  '0.001*\"challenge\" + 0.001*\"community\" + 0.001*\"projection\" + 0.001*\"state\" '\n",
      "  '+ 0.001*\"member\" + 0.001*\"side\" + 0.001*\"october\" + 0.001*\"staff\" + '\n",
      "  '0.001*\"contained\" + 0.001*\"determined\"'),\n",
      " (14,\n",
      "  '0.013*\"commodity\" + 0.010*\"tension\" + 0.008*\"policyrelevant\" + '\n",
      "  '0.008*\"anchoring\" + 0.008*\"owing\" + 0.008*\"keeping\" + 0.008*\"aim\" + '\n",
      "  '0.008*\"process\" + 0.007*\"balanced\" + 0.007*\"momentum\"'),\n",
      " (15,\n",
      "  '0.007*\"trend\" + 0.007*\"background\" + 0.007*\"food\" + 0.006*\"sound\" + '\n",
      "  '0.006*\"thus\" + 0.006*\"confirmed\" + 0.005*\"significantly\" + '\n",
      "  '0.005*\"volatility\" + 0.005*\"objective\" + 0.005*\"cost\"'),\n",
      " (16,\n",
      "  '0.030*\"banknote\" + 0.029*\"central\" + 0.026*\"pillar\" + 0.024*\"national\" + '\n",
      "  '0.022*\"member\" + 0.019*\"changeover\" + 0.017*\"state\" + 0.016*\"three\" + '\n",
      "  '0.013*\"already\" + 0.013*\"issue\"'),\n",
      " (17,\n",
      "  '0.001*\"projection\" + 0.001*\"staff\" + 0.001*\"timely\" + 0.001*\"consumer\" + '\n",
      "  '0.001*\"trend\" + 0.001*\"around\" + 0.001*\"appear\" + 0.001*\"manner\" + '\n",
      "  '0.001*\"vigilance\" + 0.001*\"reducing\"'),\n",
      " (18,\n",
      "  '0.060*\"subdued\" + 0.025*\"gradual\" + 0.021*\"picture\" + 0.019*\"long\" + '\n",
      "  '0.016*\"weak\" + 0.016*\"made\" + 0.015*\"needed\" + 0.015*\"investor\" + '\n",
      "  '0.014*\"weigh\" + 0.013*\"improving\"'),\n",
      " (19,\n",
      "  '0.107*\"operation\" + 0.072*\"refinancing\" + 0.056*\"fixed\" + 0.040*\"procedure\" '\n",
      "  '+ 0.040*\"allotment\" + 0.036*\"tender\" + 0.030*\"maintenance\" + 0.024*\"mros\" + '\n",
      "  '0.023*\"end\" + 0.019*\"specialterm\"')]\n",
      "[(49,\n",
      "  '0.001*\"day\" + 0.001*\"target\" + 0.001*\"system\" + 0.001*\"banknote\" + '\n",
      "  '0.001*\"end\" + 0.001*\"national\" + 0.001*\"reflected\" + 0.001*\"december\" + '\n",
      "  '0.001*\"two\" + 0.001*\"lower\"'),\n",
      " (22,\n",
      "  '0.001*\"two\" + 0.001*\"vigilance\" + 0.001*\"longer\" + 0.001*\"significantly\" + '\n",
      "  '0.001*\"ample\" + 0.001*\"role\" + 0.001*\"position\" + 0.001*\"temporary\" + '\n",
      "  '0.001*\"accordingly\" + 0.001*\"scenario\"'),\n",
      " (37,\n",
      "  '0.001*\"projection\" + 0.001*\"staff\" + 0.001*\"longer\" + 0.001*\"background\" + '\n",
      "  '0.001*\"range\" + 0.001*\"service\" + 0.001*\"system\" + 0.001*\"programme\" + '\n",
      "  '0.001*\"ample\" + 0.001*\"vigilance\"'),\n",
      " (42,\n",
      "  '0.001*\"central\" + 0.001*\"system\" + 0.001*\"national\" + 0.001*\"state\" + '\n",
      "  '0.001*\"within\" + 0.001*\"legal\" + 0.001*\"agreed\" + 0.001*\"framework\" + '\n",
      "  '0.001*\"reference\" + 0.001*\"reserve\"'),\n",
      " (32,\n",
      "  '0.001*\"course\" + 0.001*\"banknote\" + 0.001*\"production\" + 0.001*\"ample\" + '\n",
      "  '0.001*\"consumer\" + 0.001*\"manner\" + 0.001*\"december\" + 0.001*\"day\" + '\n",
      "  '0.001*\"system\" + 0.001*\"end\"'),\n",
      " (0,\n",
      "  '0.001*\"governance\" + 0.001*\"projection\" + 0.001*\"staff\" + 0.001*\"downwards\" '\n",
      "  '+ 0.001*\"longer\" + 0.001*\"heightened\" + 0.001*\"march\" + 0.001*\"contained\" + '\n",
      "  '0.001*\"confirmed\" + 0.001*\"below\"'),\n",
      " (24,\n",
      "  '0.001*\"trend\" + 0.001*\"ample\" + 0.001*\"longer\" + 0.001*\"thus\" + '\n",
      "  '0.001*\"robust\" + 0.001*\"prospect\" + 0.001*\"average\" + 0.001*\"job\" + '\n",
      "  '0.001*\"confirmed\" + 0.001*\"consumer\"'),\n",
      " (7,\n",
      "  '0.001*\"background\" + 0.001*\"three\" + 0.001*\"projection\" + 0.001*\"january\" + '\n",
      "  '0.001*\"around\" + 0.001*\"member\" + 0.001*\"trend\" + 0.001*\"stage\" + '\n",
      "  '0.001*\"consumer\" + 0.001*\"situation\"'),\n",
      " (31,\n",
      "  '0.001*\"consumer\" + 0.001*\"significantly\" + 0.001*\"side\" + 0.001*\"vigilance\" '\n",
      "  '+ 0.001*\"like\" + 0.001*\"finance\" + 0.001*\"start\" + 0.001*\"fact\" + '\n",
      "  '0.001*\"discussion\" + 0.001*\"member\"'),\n",
      " (20,\n",
      "  '0.001*\"exchange\" + 0.001*\"central\" + 0.001*\"national\" + 0.001*\"member\" + '\n",
      "  '0.001*\"process\" + 0.001*\"banknote\" + 0.001*\"start\" + 0.001*\"february\" + '\n",
      "  '0.001*\"framework\" + 0.001*\"world\"'),\n",
      " (47,\n",
      "  '0.328*\"tilted\" + 0.314*\"steady\" + 0.245*\"using\" + 0.035*\"pick\" + '\n",
      "  '0.000*\"purchase\" + 0.000*\"june\" + 0.000*\"meanwhile\" + 0.000*\"guidance\" + '\n",
      "  '0.000*\"compliance\" + 0.000*\"narrow\"'),\n",
      " (13,\n",
      "  '0.201*\"stepped\" + 0.200*\"mandate\" + 0.106*\"supportive\" + 0.106*\"political\" '\n",
      "  '+ 0.065*\"highly\" + 0.065*\"turned\" + 0.053*\"increasingly\" + 0.051*\"helped\" + '\n",
      "  '0.040*\"though\" + 0.019*\"again\"'),\n",
      " (29,\n",
      "  '0.210*\"boost\" + 0.123*\"resilient\" + 0.113*\"decisively\" + 0.107*\"act\" + '\n",
      "  '0.086*\"resolution\" + 0.043*\"subdued\" + 0.042*\"nonstandard\" + '\n",
      "  '0.032*\"securitisation\" + 0.030*\"contributed\" + 0.029*\"owing\"'),\n",
      " (48,\n",
      "  '0.034*\"mandate\" + 0.024*\"cycle\" + 0.021*\"heightened\" + 0.020*\"mfi\" + '\n",
      "  '0.019*\"weigh\" + 0.019*\"subdued\" + 0.018*\"securitisation\" + 0.018*\"weaker\" + '\n",
      "  '0.016*\"union\" + 0.015*\"made\"'),\n",
      " (3,\n",
      "  '0.168*\"projection\" + 0.140*\"staff\" + 0.057*\"june\" + 0.047*\"revised\" + '\n",
      "  '0.045*\"july\" + 0.040*\"slightly\" + 0.038*\"foresee\" + 0.023*\"gradually\" + '\n",
      "  '0.023*\"reflected\" + 0.020*\"march\"'),\n",
      " (12,\n",
      "  '0.015*\"course\" + 0.013*\"help\" + 0.013*\"framework\" + 0.012*\"several\" + '\n",
      "  '0.011*\"fall\" + 0.011*\"affect\" + 0.011*\"weak\" + 0.010*\"position\" + '\n",
      "  '0.009*\"needed\" + 0.009*\"finance\"'),\n",
      " (15,\n",
      "  '0.008*\"longer\" + 0.007*\"sound\" + 0.006*\"background\" + 0.006*\"thus\" + '\n",
      "  '0.006*\"volatility\" + 0.006*\"confirmed\" + 0.005*\"trend\" + 0.005*\"robust\" + '\n",
      "  '0.005*\"accordingly\" + 0.005*\"temporary\"'),\n",
      " (14,\n",
      "  '0.029*\"positively\" + 0.027*\"tension\" + 0.022*\"commodity\" + 0.021*\"august\" + '\n",
      "  '0.015*\"accommodative\" + 0.013*\"competitiveness\" + 0.012*\"july\" + '\n",
      "  '0.012*\"deposit\" + 0.012*\"owing\" + 0.011*\"negative\"'),\n",
      " (33,\n",
      "  '0.027*\"purchase\" + 0.015*\"asset\" + 0.015*\"sustained\" + 0.015*\"path\" + '\n",
      "  '0.014*\"across\" + 0.014*\"march\" + 0.013*\"contribute\" + 0.012*\"accommodative\" '\n",
      "  '+ 0.012*\"june\" + 0.010*\"including\"'),\n",
      " (5,\n",
      "  '0.022*\"net\" + 0.019*\"october\" + 0.017*\"aim\" + 0.015*\"to\" + 0.014*\"below\" + '\n",
      "  '0.013*\"september\" + 0.013*\"december\" + 0.013*\"infrastructure\" + '\n",
      "  '0.011*\"incoming\" + 0.011*\"adjusted\"')]\n",
      "[(65,\n",
      "  '0.001*\"productivity\" + 0.001*\"projection\" + 0.001*\"deficit\" + '\n",
      "  '0.001*\"vigilance\" + 0.001*\"staff\" + 0.001*\"excessive\" + 0.001*\"side\" + '\n",
      "  '0.001*\"use\" + 0.001*\"projected\" + 0.001*\"trend\"'),\n",
      " (88,\n",
      "  '0.001*\"sustainability\" + 0.001*\"effort\" + 0.001*\"world\" + 0.001*\"gradual\" + '\n",
      "  '0.001*\"budgetary\" + 0.001*\"related\" + 0.001*\"lisbon\" + 0.001*\"generally\" + '\n",
      "  '0.001*\"reason\" + 0.001*\"respect\"'),\n",
      " (32,\n",
      "  '0.001*\"degree\" + 0.001*\"objective\" + 0.001*\"consumer\" + 0.001*\"world\" + '\n",
      "  '0.001*\"improve\" + 0.001*\"geopolitical\" + 0.001*\"hand\" + 0.001*\"course\" + '\n",
      "  '0.001*\"fully\" + 0.001*\"indication\"'),\n",
      " (22,\n",
      "  '0.001*\"gradual\" + 0.001*\"sound\" + 0.001*\"due\" + 0.001*\"reason\" + '\n",
      "  '0.001*\"early\" + 0.001*\"expenditure\" + 0.001*\"related\" + 0.001*\"position\" + '\n",
      "  '0.001*\"side\" + 0.001*\"generally\"'),\n",
      " (57,\n",
      "  '0.001*\"projection\" + 0.001*\"deficit\" + 0.001*\"gradual\" + 0.001*\"excessive\" '\n",
      "  '+ 0.001*\"noted\" + 0.001*\"staff\" + 0.001*\"due\" + 0.001*\"ecofin\" + '\n",
      "  '0.001*\"next\" + 0.001*\"scenario\"'),\n",
      " (80,\n",
      "  '0.001*\"state\" + 0.001*\"system\" + 0.001*\"member\" + 0.001*\"vigilance\" + '\n",
      "  '0.001*\"finance\" + 0.001*\"agreed\" + 0.001*\"central\" + 0.001*\"definition\" + '\n",
      "  '0.001*\"statistic\" + 0.001*\"consumer\"'),\n",
      " (8,\n",
      "  '0.001*\"projection\" + 0.001*\"ample\" + 0.001*\"staff\" + 0.001*\"contribution\" + '\n",
      "  '0.001*\"range\" + 0.001*\"longer\" + 0.001*\"confirmed\" + 0.001*\"projected\" + '\n",
      "  '0.001*\"creation\" + 0.001*\"revised\"'),\n",
      " (68,\n",
      "  '0.001*\"system\" + 0.001*\"april\" + 0.001*\"exchange\" + 0.001*\"central\" + '\n",
      "  '0.001*\"national\" + 0.001*\"january\" + 0.001*\"operation\" + 0.001*\"world\" + '\n",
      "  '0.001*\"banknote\" + 0.001*\"day\"'),\n",
      " (52,\n",
      "  '0.001*\"pillar\" + 0.001*\"moderation\" + 0.001*\"state\" + 0.001*\"position\" + '\n",
      "  '0.001*\"background\" + 0.001*\"shock\" + 0.001*\"asset\" + 0.001*\"many\" + '\n",
      "  '0.001*\"underpin\" + 0.001*\"international\"'),\n",
      " (77,\n",
      "  '0.001*\"day\" + 0.001*\"banknote\" + 0.001*\"internal\" + 0.001*\"production\" + '\n",
      "  '0.001*\"occurred\" + 0.001*\"pointing\" + 0.001*\"unprocessed\" + 0.001*\"target\" '\n",
      "  '+ 0.001*\"consumer\" + 0.001*\"central\"'),\n",
      " (18,\n",
      "  '0.026*\"world\" + 0.025*\"finance\" + 0.022*\"gradual\" + 0.021*\"framework\" + '\n",
      "  '0.017*\"treaty\" + 0.017*\"forecast\" + 0.016*\"social\" + 0.016*\"shift\" + '\n",
      "  '0.016*\"fall\" + 0.015*\"crosschecking\"'),\n",
      " (4,\n",
      "  '0.028*\"operation\" + 0.020*\"fixed\" + 0.019*\"consumer\" + 0.018*\"refinancing\" '\n",
      "  '+ 0.016*\"threemonth\" + 0.016*\"maintenance\" + 0.016*\"facility\" + '\n",
      "  '0.015*\"value\" + 0.014*\"tender\" + 0.014*\"reference\"'),\n",
      " (1,\n",
      "  '0.331*\"enterprise\" + 0.091*\"scheme\" + 0.076*\"helped\" + 0.076*\"encouraging\" '\n",
      "  '+ 0.043*\"cost\" + 0.035*\"burden\" + 0.030*\"central\" + 0.027*\"remuneration\" + '\n",
      "  '0.025*\"slowing\" + 0.024*\"robust\"'),\n",
      " (67,\n",
      "  '0.141*\"projection\" + 0.099*\"staff\" + 0.069*\"december\" + 0.058*\"range\" + '\n",
      "  '0.046*\"october\" + 0.044*\"security\" + 0.034*\"foresee\" + 0.028*\"january\" + '\n",
      "  '0.028*\"reflected\" + 0.024*\"fourth\"'),\n",
      " (30,\n",
      "  '0.150*\"tilted\" + 0.097*\"baseline\" + 0.081*\"scenario\" + 0.065*\"prevailing\" + '\n",
      "  '0.062*\"supportive\" + 0.060*\"flow\" + 0.051*\"robust\" + 0.039*\"contained\" + '\n",
      "  '0.036*\"plan\" + 0.032*\"eu\"'),\n",
      " (71,\n",
      "  '0.078*\"purchase\" + 0.071*\"march\" + 0.055*\"sustained\" + 0.037*\"asset\" + '\n",
      "  '0.034*\"emphasised\" + 0.025*\"beyond\" + 0.025*\"raise\" + 0.025*\"corporate\" + '\n",
      "  '0.023*\"effective\" + 0.023*\"programme\"'),\n",
      " (79,\n",
      "  '0.011*\"anchoring\" + 0.010*\"competitiveness\" + 0.010*\"policyrelevant\" + '\n",
      "  '0.010*\"longer\" + 0.009*\"deficit\" + 0.009*\"accordingly\" + '\n",
      "  '0.009*\"competition\" + 0.009*\"firm\" + 0.008*\"volatility\" + 0.008*\"side\"'),\n",
      " (93,\n",
      "  '0.033*\"secure\" + 0.029*\"to\" + 0.027*\"gradual\" + 0.023*\"composition\" + '\n",
      "  '0.022*\"instrument\" + 0.022*\"aim\" + 0.022*\"july\" + 0.021*\"asset\" + '\n",
      "  '0.019*\"accommodation\" + 0.018*\"cycle\"'),\n",
      " (97,\n",
      "  '0.021*\"june\" + 0.020*\"purchase\" + 0.019*\"staff\" + 0.016*\"path\" + '\n",
      "  '0.016*\"contribute\" + 0.015*\"full\" + 0.015*\"projection\" + '\n",
      "  '0.014*\"accommodative\" + 0.013*\"april\" + 0.013*\"across\"'),\n",
      " (5,\n",
      "  '0.009*\"act\" + 0.009*\"september\" + 0.009*\"using\" + 0.008*\"repeatedly\" + '\n",
      "  '0.008*\"incoming\" + 0.008*\"aim\" + 0.007*\"made\" + 0.007*\"expanding\" + '\n",
      "  '0.007*\"resolution\" + 0.007*\"deposit\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model5.print_topics())\n",
    "doc_lda = lda_model5[corpus]\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model10.print_topics())\n",
    "doc_lda = lda_model10[corpus]\n",
    "\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model20.print_topics())\n",
    "doc_lda = lda_model20[corpus]\n",
    "\n",
    "# Print the Keyword in the 50 topics\n",
    "pprint(lda_model50.print_topics())\n",
    "doc_lda = lda_model50[corpus]\n",
    "\n",
    "# Print the Keyword in the 100 topics\n",
    "pprint(lda_model100.print_topics())\n",
    "doc_lda = lda_model100[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.971190836614309\n",
      "\n",
      "Perplexity5:  -6.741949630076177\n",
      "\n",
      "Perplexity10:  -6.731808115090489\n",
      "\n",
      "Perplexity20:  -6.7951167265408605\n",
      "\n",
      "Perplexity50:  -6.843762759453129\n",
      "\n",
      "Perplexity100:  -8.95184924229589\n",
      "\n",
      "Coherence Score:  0.32373654645368644\n",
      "\n",
      "Coherence Score5:  0.35288285743955267\n",
      "\n",
      "Coherence Score10:  0.3656706655865734\n",
      "\n",
      "Coherence Score20:  0.3447703244614714\n",
      "\n",
      "Coherence Score50:  0.3373415190364386\n",
      "\n",
      "Coherence Score100:  0.30821028741538675\n",
      "\n",
      "Coherence ScoreUMass:  -0.7498682561370543\n",
      "\n",
      "Coherence Score5UMass:  -0.8789431941538824\n",
      "\n",
      "Coherence Score10UMass:  -0.8763090784230287\n",
      "\n",
      "Coherence Score20UMass:  -1.0379848895412782\n",
      "\n",
      "Coherence Score50UMass:  -1.194463698780142\n",
      "\n",
      "Coherence Score100UMass:  -1.205081098109926\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity5: ', lda_model5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity10: ', lda_model10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity20: ', lda_model20.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity50: ', lda_model50.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity100: ', lda_model100.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100: ', coherence_lda)\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence ScoreUMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model5, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score5UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model10, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score10UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model20, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score20UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model50, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score50UMass: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model100, texts=data, dictionary=id2word, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score100UMass: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
